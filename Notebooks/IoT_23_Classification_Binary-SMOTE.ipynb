{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_T2LCfAN_XF"
   },
   "source": [
    "**Library Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "K1li4zpPN_XH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from IPython.display import display\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0t7965zN_XH"
   },
   "source": [
    "Panda Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "brXljTxkN_XI"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoPy7wczN_XI"
   },
   "source": [
    "Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YM-QslAnN_XI"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Datasets/flow-farm_train_smote.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-463164356cd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../Datasets/flow-farm_train_smote.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Datasets/flow-farm_train_smote.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Datasets/flow-farm_train_smote.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xEU9UPRN_XI"
   },
   "source": [
    "Fix Dataframe Mixed Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SDKSZ_89N_XJ"
   },
   "outputs": [],
   "source": [
    "# Remove rows with '-' character in columns 7, 8 and 9\n",
    "cols_to_check = ['duration', 'orig_bytes', 'resp_bytes']\n",
    "#cols_to_check = ['duration', 'orig_bytes', 'resp_bytes', 'conn_state', 'history', 'flow_duration']\n",
    "\n",
    "mask = df[cols_to_check].apply(lambda x: x.str.contains('-', na=False)).any(axis=1)\n",
    "df = df[~mask]\n",
    "\n",
    "# Replace comma with period as decimal separator\n",
    "\n",
    "cols_to_float = ['duration']\n",
    "df[cols_to_float] = df[cols_to_float].replace(',', '.', regex=True)\n",
    "\n",
    "# Convert columns 7, 8, 9, and 17 to float and int data type\n",
    "cols_to_int = ['orig_bytes', 'resp_bytes']\n",
    "\n",
    "df[cols_to_float] = df[cols_to_float].astype(float)\n",
    "df[cols_to_int] = df[cols_to_int].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3OxjJEb6fxi"
   },
   "source": [
    "-----------------------------------------------------------\n",
    "\n",
    "**DF Statistics and Info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljdybTvfN_XJ"
   },
   "outputs": [],
   "source": [
    "def display_information_dataframe(df_cop):\n",
    "    summary_data = [{'Data Type': dtype, 'Column Name': col, 'Unique Values': df_cop[col].unique()} for col, dtype in df_cop.dtypes.iteritems()]\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    pd.options.display.max_rows = None\n",
    "    pd.options.display.max_columns = None\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4BJ-3JT66fxi",
    "outputId": "38be4775-5564-4772-f561-ed34f559bdd7"
   },
   "outputs": [],
   "source": [
    "display_information_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgb20-msN_XI"
   },
   "source": [
    "--------------------------------------------\n",
    "\n",
    "**Pre-processing and Data Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2L5yxSG6fxk"
   },
   "source": [
    "Split History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byGEDcyWZtM7"
   },
   "outputs": [],
   "source": [
    "def count_letters(string, is_upper):\n",
    "    count = 0\n",
    "    for c in string:\n",
    "        if is_upper and c.isupper():\n",
    "            count += 1\n",
    "        elif not is_upper and c.islower():\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGQEQC_GbWZD"
   },
   "outputs": [],
   "source": [
    "df['history_originator'] = df['history'].apply(lambda x: count_letters(x, True))\n",
    "df['history_responder'] = df['history'].apply(lambda x: count_letters(x, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6MG8V_sC7y3"
   },
   "source": [
    "One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "psGbkLKcFqA_"
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(df, columns):\n",
    "    for col in columns:\n",
    "        print(f'[ONE HOT ENCONDING] {col}')\n",
    "        df = pd.get_dummies(df, columns=[col], prefix=col)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MoAIe3cv9mE-",
    "outputId": "55a1eb68-aac9-4987-87f3-1e3dbbca521e"
   },
   "outputs": [],
   "source": [
    "cols_to_encode = [\n",
    "    'proto',\n",
    "    'conn_state',\n",
    "    'fwd_header_size_min',\n",
    "    'fwd_header_size_max',\n",
    "    'bwd_header_size_min',\n",
    "    'bwd_header_size_max',\n",
    "    'flow_FIN_flag_count',\n",
    "    'flow_SYN_flag_count',\n",
    "    'flow_RST_flag_count',\n",
    "    'history_originator',\n",
    "    'history_responder',\n",
    "]\n",
    "\n",
    "df = one_hot_encoding(df,cols_to_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvG-Y20AN_XM"
   },
   "outputs": [],
   "source": [
    "def missed_bytes(missed_bytes):\n",
    "    if missed_bytes < 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "GrZK4p3rN_XM",
    "outputId": "bb422da6-cb67-42d5-d669-7a06a4b15ab7"
   },
   "outputs": [],
   "source": [
    "df['missed_bytes'] = df.apply(lambda row: missed_bytes(row['missed_bytes']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSbkiI6JD6vO"
   },
   "source": [
    "Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df,columns,n_std):\n",
    "    for col in columns:\n",
    "        print(f'[REMOVE OUTLIERS] {col}')\n",
    "        \n",
    "        mean = df[col].mean()\n",
    "        sd = df[col].std()\n",
    "        \n",
    "        df = df[(df[col] <= mean+(n_std*sd))]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7G5NkFuBD8lM"
   },
   "outputs": [],
   "source": [
    "outliers = [\n",
    "    'orig_pkts',\n",
    "    'resp_pkts',\n",
    "    'orig_ip_bytes',\n",
    "    'resp_ip_bytes',\n",
    "]\n",
    "\n",
    "df = remove_outliers(df, outliers, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsIbHSu0N_XK"
   },
   "source": [
    "Normalize, Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s68skGdoN_XK"
   },
   "outputs": [],
   "source": [
    "def zscore_normalization(df, cols):\n",
    "    # Standardize the selected columns\n",
    "    for col in cols:\n",
    "        if col not in df.columns:\n",
    "            print(f\"[WARNING] {col} not found in DataFrame.\")\n",
    "            continue\n",
    "        df[col] = zscore(df[col])\n",
    "    \n",
    "    print(\"[DONE] Z-score Normalization\")\n",
    "    print(\"[INFO] Current Fields in the DataFrame:\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "nQdNGsJ-N_XK",
    "outputId": "3721d8a8-a9a0-491f-ddd1-da08e37c1e89",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols_to_zscore = [\n",
    "    'flow_duration', 'fwd_pkts_tot', 'bwd_pkts_tot', 'fwd_data_pkts_tot',\n",
    "    'bwd_data_pkts_tot', 'fwd_pkts_per_sec', 'bwd_pkts_per_sec',\n",
    "    'flow_pkts_per_sec', 'down_up_ratio', 'fwd_header_size_tot', \n",
    "    'bwd_header_size_tot', 'fwd_PSH_flag_count',\n",
    "    'bwd_PSH_flag_count', 'flow_ACK_flag_count', 'fwd_pkts_payload.min',\n",
    "    'fwd_pkts_payload.max', 'fwd_pkts_payload.tot', 'fwd_pkts_payload.avg',\n",
    "    'fwd_pkts_payload.std', 'bwd_pkts_payload.min', 'bwd_pkts_payload.max',\n",
    "    'bwd_pkts_payload.tot', 'bwd_pkts_payload.avg', 'bwd_pkts_payload.std',\n",
    "    'flow_pkts_payload.min', 'flow_pkts_payload.max', 'flow_pkts_payload.tot',\n",
    "    'flow_pkts_payload.avg', 'flow_pkts_payload.std', 'fwd_iat.min',\n",
    "    'fwd_iat.max', 'fwd_iat.tot', 'fwd_iat.avg', 'fwd_iat.std',\n",
    "    'bwd_iat.min', 'bwd_iat.max', 'bwd_iat.tot', 'bwd_iat.avg', 'bwd_iat.std',\n",
    "    'flow_iat.min', 'flow_iat.max', 'flow_iat.tot', 'flow_iat.avg',\n",
    "    'flow_iat.std', 'payload_bytes_per_second', 'fwd_subflow_pkts',\n",
    "    'bwd_subflow_pkts', 'fwd_subflow_bytes', 'bwd_subflow_bytes',\n",
    "    'fwd_bulk_bytes', 'bwd_bulk_bytes', 'fwd_bulk_packets', 'bwd_bulk_packets',\n",
    "    'fwd_bulk_rate', 'bwd_bulk_rate', 'active.max', 'active.tot',\n",
    "    'active.avg', 'active.std', 'idle.min', 'idle.max', 'idle.tot',\n",
    "    'idle.avg', 'idle.std', 'fwd_init_window_size', 'bwd_init_window_size',\n",
    "    'fwd_last_window_size', 'bwd_last_window_size', 'duration', 'orig_bytes',\n",
    "    'resp_bytes', 'orig_pkts', 'resp_pkts', 'resp_ip_bytes', 'orig_ip_bytes',\n",
    "]\n",
    "\n",
    "df = zscore_normalization(df, cols_to_zscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcHY3Kk_N_XN"
   },
   "source": [
    "---------------------------------------\n",
    "\n",
    "**Create Model & Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9A5Vaq8Naa2"
   },
   "outputs": [],
   "source": [
    "df['is_attack'] = df['type'].apply(lambda x: 0 if x == \"normal\" else 1)\n",
    "df.groupby('is_attack')['is_attack'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paLhEkkLN_XK"
   },
   "source": [
    "Delete Insignificant Columns from the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gM8kKWVhN_XK"
   },
   "outputs": [],
   "source": [
    "def delete_columns(df, cols):\n",
    "    for col in cols:\n",
    "        df.drop(col, axis = 1, inplace = True)\n",
    "        print(f'[REMOVED] {col}')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1LC6qBPhN_XK",
    "outputId": "ae158dcd-20b4-4f09-9bf6-63e4ba623aa5"
   },
   "outputs": [],
   "source": [
    "cols_to_del = [\n",
    "    'uid',\n",
    "    'id.orig_h',\n",
    "    'id.orig_p',\n",
    "    'id.resp_h',\n",
    "    'id.resp_p',\n",
    "    'active.min',\n",
    "    'service',\n",
    "    'history',\n",
    "    'type',\n",
    "    'local_orig',\n",
    "    'local_resp',\n",
    "    'tunnel_parents',\n",
    "    'fwd_URG_flag_count',\n",
    "    'bwd_URG_flag_count',\n",
    "    'flow_CWR_flag_count',\n",
    "    'flow_ECE_flag_count',\n",
    "    ]\n",
    "\n",
    "df = delete_columns(df,cols_to_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7ExpPs6N_XN"
   },
   "outputs": [],
   "source": [
    "# Split into input and output variables\n",
    "x_columns = df.columns.drop('is_attack')\n",
    "dummies = pd.get_dummies(df['is_attack'])\n",
    "x = df[x_columns].values\n",
    "attack = dummies.columns\n",
    "y = dummies.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE()\n",
    "x_train_smote, y_train_smote = oversample.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JllGCOC76fxm"
   },
   "outputs": [],
   "source": [
    "x_train_smote, x_test_smote, y_train_smote, y_test_smote = train_test_split(x_train_smote, y_train_smote, test_size=0.25, random_state=42, stratify=y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_smote.shape,y_train_smote.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmWyRAR0N_XN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "\n",
    "# Define early stopping\n",
    "monitor = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\",factor=0.5,mode=\"min\",patience=10,verbose=1,min_lr=1e-7)\n",
    "checkpoint = ModelCheckpoint('best_model_binary.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train_smote, y_train_smote, validation_data=(x_test_smote, y_test_smote), epochs=100, batch_size=512, callbacks=[monitor, checkpoint])\n",
    "\n",
    "# Load the best saved model\n",
    "best_model = load_model('best_model_binary.h5')\n",
    "\n",
    "# Evaluate the best saved model\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "score = best_model.evaluate(x_test, y_test)\n",
    "print('')\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 16))\n",
    "\n",
    "ax.plot(history.history['loss'], label='train')\n",
    "ax.plot(history.history['val_loss'], label='test')\n",
    "ax.set_title('Model Loss')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5hVv8L2EN_XN",
    "outputId": "6d8a189e-7051-42a6-dd0d-b626eb55d212"
   },
   "outputs": [],
   "source": [
    "pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjwPGzH7cXls"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred, y_test):\n",
    "    y_pred = np.round(pred).astype(int)\n",
    "    \n",
    "    correct = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {correct}\")\n",
    "    \n",
    "    recall = metrics.recall_score(y_test, y_pred, average = 'weighted')    \n",
    "    print(f\"Recall: {recall}\")\n",
    "       \n",
    "    precision = metrics.precision_score(y_test, y_pred, average = 'weighted')\n",
    "    print(f\"Precision: {precision}\")\n",
    "    \n",
    "    f1score = metrics.f1_score(y_test, y_pred, average = 'weighted')\n",
    "    print(f\"F1Score: {f1score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7opqgASr6fxn"
   },
   "outputs": [],
   "source": [
    "compute_metrics(pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ihn2qm186fxn"
   },
   "source": [
    "-------------------------------------\n",
    "\n",
    "**Result Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7oqLlxmicPrp"
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "y_pred = np.round(pred).astype(int)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display confusion matrix\n",
    "cmd = ConfusionMatrixDisplay(cm)\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "cmd.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_za--boceRr9"
   },
   "outputs": [],
   "source": [
    "# Usage of ExtraTreesClassifier for feature selection\n",
    "extra_tree_forest = ExtraTreesClassifier(n_estimators = 5, criterion ='entropy', max_features = 2)\n",
    "extra_tree_forest.fit(x, y)\n",
    "feature_importance = extra_tree_forest.feature_importances_\n",
    "feature_importance_normalized = np.std([tree.feature_importances_ for tree in  extra_tree_forest.estimators_], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBfWThvfeTNB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plor for the ExtraTreesClassifier output\n",
    "plot.bar(x_columns, feature_importance_normalized)\n",
    "plot.xlabel('Feature Labels')\n",
    "plot.ylabel('Feature Importances')\n",
    "plot.title('Comparison of different feature importances in the current dataset')\n",
    "plot.xticks(rotation = 90)\n",
    "\n",
    "# Plot size\n",
    "plot.rcParams[\"figure.figsize\"] = (70, 40)\n",
    "\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
