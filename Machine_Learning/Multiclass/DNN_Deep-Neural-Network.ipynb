{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_T2LCfAN_XF"
   },
   "source": [
    "**Library Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "K1li4zpPN_XH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from IPython.display import display\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0t7965zN_XH"
   },
   "source": [
    "Panda Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "brXljTxkN_XI"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoPy7wczN_XI"
   },
   "source": [
    "Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YM-QslAnN_XI",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missed_bytes</th>\n",
       "      <th>orig_pkts</th>\n",
       "      <th>orig_ip_bytes</th>\n",
       "      <th>resp_pkts</th>\n",
       "      <th>resp_ip_bytes</th>\n",
       "      <th>fwd_pkts_per_sec</th>\n",
       "      <th>bwd_pkts_per_sec</th>\n",
       "      <th>flow_pkts_per_sec</th>\n",
       "      <th>down_up_ratio</th>\n",
       "      <th>fwd_header_size_tot</th>\n",
       "      <th>fwd_header_size_min</th>\n",
       "      <th>fwd_header_size_max</th>\n",
       "      <th>bwd_header_size_tot</th>\n",
       "      <th>bwd_header_size_min</th>\n",
       "      <th>bwd_header_size_max</th>\n",
       "      <th>fwd_pkts_payload.tot</th>\n",
       "      <th>fwd_pkts_payload.avg</th>\n",
       "      <th>bwd_pkts_payload.tot</th>\n",
       "      <th>bwd_pkts_payload.avg</th>\n",
       "      <th>flow_pkts_payload.tot</th>\n",
       "      <th>flow_pkts_payload.avg</th>\n",
       "      <th>fwd_iat.tot</th>\n",
       "      <th>fwd_iat.avg</th>\n",
       "      <th>bwd_iat.tot</th>\n",
       "      <th>bwd_iat.avg</th>\n",
       "      <th>flow_iat.tot</th>\n",
       "      <th>flow_iat.avg</th>\n",
       "      <th>traffic</th>\n",
       "      <th>pkts_difference</th>\n",
       "      <th>data_pkts_difference</th>\n",
       "      <th>is_attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067841</td>\n",
       "      <td>0.005959</td>\n",
       "      <td>0.881829</td>\n",
       "      <td>1.503112</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>192</td>\n",
       "      <td>1.566169</td>\n",
       "      <td>1.478170</td>\n",
       "      <td>256</td>\n",
       "      <td>2.712843</td>\n",
       "      <td>4.268354</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>1.589631</td>\n",
       "      <td>6.736335</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>0.829308</td>\n",
       "      <td>0.245222</td>\n",
       "      <td>1.220854</td>\n",
       "      <td>2.096534</td>\n",
       "      <td>0.824683</td>\n",
       "      <td>0.006424</td>\n",
       "      <td>HTTP_Flood</td>\n",
       "      <td>-0.094869</td>\n",
       "      <td>0.139609</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067841</td>\n",
       "      <td>-0.237735</td>\n",
       "      <td>-0.500297</td>\n",
       "      <td>-0.536844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24</td>\n",
       "      <td>0.655956</td>\n",
       "      <td>0.605343</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.444625</td>\n",
       "      <td>-0.434283</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>-0.052290</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.159412</td>\n",
       "      <td>-0.168388</td>\n",
       "      <td>-0.076153</td>\n",
       "      <td>-0.096974</td>\n",
       "      <td>-0.163665</td>\n",
       "      <td>-0.167367</td>\n",
       "      <td>Port_Scanning</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>-0.013989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067841</td>\n",
       "      <td>-0.393700</td>\n",
       "      <td>-0.500297</td>\n",
       "      <td>-0.536844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>-1.164469</td>\n",
       "      <td>-1.140311</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.444625</td>\n",
       "      <td>-0.434283</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>-0.052290</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.159412</td>\n",
       "      <td>-0.168388</td>\n",
       "      <td>-0.076153</td>\n",
       "      <td>-0.096974</td>\n",
       "      <td>-0.163665</td>\n",
       "      <td>-0.167367</td>\n",
       "      <td>UDP_Flood</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>-0.013989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067841</td>\n",
       "      <td>-0.393700</td>\n",
       "      <td>-0.500297</td>\n",
       "      <td>-0.536844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>-1.164469</td>\n",
       "      <td>-1.140311</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.444625</td>\n",
       "      <td>-0.434283</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>-0.052290</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.159412</td>\n",
       "      <td>-0.168388</td>\n",
       "      <td>-0.076153</td>\n",
       "      <td>-0.096974</td>\n",
       "      <td>-0.163665</td>\n",
       "      <td>-0.167367</td>\n",
       "      <td>UDP_Flood</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>-0.013989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067841</td>\n",
       "      <td>-0.393700</td>\n",
       "      <td>-0.500297</td>\n",
       "      <td>-0.536844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>-1.164469</td>\n",
       "      <td>-1.140311</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.444625</td>\n",
       "      <td>-0.434283</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>-0.052290</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.159412</td>\n",
       "      <td>-0.168388</td>\n",
       "      <td>-0.076153</td>\n",
       "      <td>-0.096974</td>\n",
       "      <td>-0.163665</td>\n",
       "      <td>-0.167367</td>\n",
       "      <td>UDP_Flood</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>-0.013989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839594</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067841</td>\n",
       "      <td>-0.276726</td>\n",
       "      <td>-0.500297</td>\n",
       "      <td>-0.536844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.200850</td>\n",
       "      <td>0.168930</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.444625</td>\n",
       "      <td>-0.434283</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>-0.052290</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.159412</td>\n",
       "      <td>-0.168388</td>\n",
       "      <td>-0.076153</td>\n",
       "      <td>-0.096974</td>\n",
       "      <td>-0.163665</td>\n",
       "      <td>-0.167367</td>\n",
       "      <td>TCP_Flood</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>-0.013989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839595</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067841</td>\n",
       "      <td>-0.276726</td>\n",
       "      <td>-0.500297</td>\n",
       "      <td>-0.536844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.200850</td>\n",
       "      <td>0.168930</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.444625</td>\n",
       "      <td>-0.434283</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>-0.052290</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.159412</td>\n",
       "      <td>-0.168388</td>\n",
       "      <td>-0.076153</td>\n",
       "      <td>-0.096974</td>\n",
       "      <td>-0.163665</td>\n",
       "      <td>-0.167367</td>\n",
       "      <td>TCP_Flood</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>-0.013989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839596</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067841</td>\n",
       "      <td>-0.276726</td>\n",
       "      <td>-0.500297</td>\n",
       "      <td>-0.536844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.200850</td>\n",
       "      <td>0.168930</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.444625</td>\n",
       "      <td>-0.434283</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>-0.052290</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.159412</td>\n",
       "      <td>-0.168388</td>\n",
       "      <td>-0.076153</td>\n",
       "      <td>-0.096974</td>\n",
       "      <td>-0.163665</td>\n",
       "      <td>-0.167367</td>\n",
       "      <td>ICMP_Flood</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>-0.013989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839597</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067841</td>\n",
       "      <td>-0.393700</td>\n",
       "      <td>-0.500297</td>\n",
       "      <td>-0.536844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>-1.164469</td>\n",
       "      <td>-1.140311</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.444625</td>\n",
       "      <td>-0.434283</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>-0.052290</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.159412</td>\n",
       "      <td>-0.168388</td>\n",
       "      <td>-0.076153</td>\n",
       "      <td>-0.096974</td>\n",
       "      <td>-0.163665</td>\n",
       "      <td>-0.167367</td>\n",
       "      <td>UDP_Flood</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>-0.013989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839598</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067841</td>\n",
       "      <td>-0.393700</td>\n",
       "      <td>-0.500297</td>\n",
       "      <td>-0.536844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>-1.164469</td>\n",
       "      <td>-1.140311</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.444625</td>\n",
       "      <td>-0.434283</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>-0.052290</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.159412</td>\n",
       "      <td>-0.168388</td>\n",
       "      <td>-0.076153</td>\n",
       "      <td>-0.096974</td>\n",
       "      <td>-0.163665</td>\n",
       "      <td>-0.167367</td>\n",
       "      <td>UDP_Flood</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>-0.013989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>839599 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        missed_bytes  orig_pkts  orig_ip_bytes  resp_pkts  resp_ip_bytes  \\\n",
       "0                0.0  -0.067841       0.005959   0.881829       1.503112   \n",
       "1                0.0  -0.067841      -0.237735  -0.500297      -0.536844   \n",
       "2                0.0  -0.067841      -0.393700  -0.500297      -0.536844   \n",
       "3                0.0  -0.067841      -0.393700  -0.500297      -0.536844   \n",
       "4                0.0  -0.067841      -0.393700  -0.500297      -0.536844   \n",
       "...              ...        ...            ...        ...            ...   \n",
       "839594           0.0  -0.067841      -0.276726  -0.500297      -0.536844   \n",
       "839595           0.0  -0.067841      -0.276726  -0.500297      -0.536844   \n",
       "839596           0.0  -0.067841      -0.276726  -0.500297      -0.536844   \n",
       "839597           0.0  -0.067841      -0.393700  -0.500297      -0.536844   \n",
       "839598           0.0  -0.067841      -0.393700  -0.500297      -0.536844   \n",
       "\n",
       "        fwd_pkts_per_sec  bwd_pkts_per_sec  flow_pkts_per_sec  down_up_ratio  \\\n",
       "0               0.000026          0.000041           0.000038       0.194444   \n",
       "1               0.000000          0.000000           0.000000       0.000000   \n",
       "2               0.000000          0.000000           0.000000       0.000000   \n",
       "3               0.000000          0.000000           0.000000       0.000000   \n",
       "4               0.000000          0.000000           0.000000       0.000000   \n",
       "...                  ...               ...                ...            ...   \n",
       "839594          0.000000          0.000000           0.000000       0.000000   \n",
       "839595          0.000000          0.000000           0.000000       0.000000   \n",
       "839596          0.000000          0.000000           0.000000       0.000000   \n",
       "839597          0.000000          0.000000           0.000000       0.000000   \n",
       "839598          0.000000          0.000000           0.000000       0.000000   \n",
       "\n",
       "        fwd_header_size_tot  fwd_header_size_min  fwd_header_size_max  \\\n",
       "0                       192             1.566169             1.478170   \n",
       "1                        24             0.655956             0.605343   \n",
       "2                         8            -1.164469            -1.140311   \n",
       "3                         8            -1.164469            -1.140311   \n",
       "4                         8            -1.164469            -1.140311   \n",
       "...                     ...                  ...                  ...   \n",
       "839594                   20             0.200850             0.168930   \n",
       "839595                   20             0.200850             0.168930   \n",
       "839596                   20             0.200850             0.168930   \n",
       "839597                    8            -1.164469            -1.140311   \n",
       "839598                    8            -1.164469            -1.140311   \n",
       "\n",
       "        bwd_header_size_tot  bwd_header_size_min  bwd_header_size_max  \\\n",
       "0                       256             2.712843             4.268354   \n",
       "1                         0            -0.444625            -0.434283   \n",
       "2                         0            -0.444625            -0.434283   \n",
       "3                         0            -0.444625            -0.434283   \n",
       "4                         0            -0.444625            -0.434283   \n",
       "...                     ...                  ...                  ...   \n",
       "839594                    0            -0.444625            -0.434283   \n",
       "839595                    0            -0.444625            -0.434283   \n",
       "839596                    0            -0.444625            -0.434283   \n",
       "839597                    0            -0.444625            -0.434283   \n",
       "839598                    0            -0.444625            -0.434283   \n",
       "\n",
       "        fwd_pkts_payload.tot  fwd_pkts_payload.avg  bwd_pkts_payload.tot  \\\n",
       "0                  -0.000945             -0.000945              1.589631   \n",
       "1                  -0.000945             -0.000945             -0.019102   \n",
       "2                  -0.000945             -0.000945             -0.019102   \n",
       "3                  -0.000945             -0.000945             -0.019102   \n",
       "4                  -0.000945             -0.000945             -0.019102   \n",
       "...                      ...                   ...                   ...   \n",
       "839594             -0.000945             -0.000945             -0.019102   \n",
       "839595             -0.000945             -0.000945             -0.019102   \n",
       "839596             -0.000945             -0.000945             -0.019102   \n",
       "839597             -0.000945             -0.000945             -0.019102   \n",
       "839598             -0.000945             -0.000945             -0.019102   \n",
       "\n",
       "        bwd_pkts_payload.avg  flow_pkts_payload.tot  flow_pkts_payload.avg  \\\n",
       "0                   6.736335              -0.000945              -0.000945   \n",
       "1                  -0.052290              -0.000945              -0.000945   \n",
       "2                  -0.052290              -0.000945              -0.000945   \n",
       "3                  -0.052290              -0.000945              -0.000945   \n",
       "4                  -0.052290              -0.000945              -0.000945   \n",
       "...                      ...                    ...                    ...   \n",
       "839594             -0.052290              -0.000945              -0.000945   \n",
       "839595             -0.052290              -0.000945              -0.000945   \n",
       "839596             -0.052290              -0.000945              -0.000945   \n",
       "839597             -0.052290              -0.000945              -0.000945   \n",
       "839598             -0.052290              -0.000945              -0.000945   \n",
       "\n",
       "        fwd_iat.tot  fwd_iat.avg  bwd_iat.tot  bwd_iat.avg  flow_iat.tot  \\\n",
       "0          0.829308     0.245222     1.220854     2.096534      0.824683   \n",
       "1         -0.159412    -0.168388    -0.076153    -0.096974     -0.163665   \n",
       "2         -0.159412    -0.168388    -0.076153    -0.096974     -0.163665   \n",
       "3         -0.159412    -0.168388    -0.076153    -0.096974     -0.163665   \n",
       "4         -0.159412    -0.168388    -0.076153    -0.096974     -0.163665   \n",
       "...             ...          ...          ...          ...           ...   \n",
       "839594    -0.159412    -0.168388    -0.076153    -0.096974     -0.163665   \n",
       "839595    -0.159412    -0.168388    -0.076153    -0.096974     -0.163665   \n",
       "839596    -0.159412    -0.168388    -0.076153    -0.096974     -0.163665   \n",
       "839597    -0.159412    -0.168388    -0.076153    -0.096974     -0.163665   \n",
       "839598    -0.159412    -0.168388    -0.076153    -0.096974     -0.163665   \n",
       "\n",
       "        flow_iat.avg        traffic  pkts_difference  data_pkts_difference  \\\n",
       "0           0.006424     HTTP_Flood        -0.094869              0.139609   \n",
       "1          -0.167367  Port_Scanning         0.007982             -0.013989   \n",
       "2          -0.167367      UDP_Flood         0.007982             -0.013989   \n",
       "3          -0.167367      UDP_Flood         0.007982             -0.013989   \n",
       "4          -0.167367      UDP_Flood         0.007982             -0.013989   \n",
       "...              ...            ...              ...                   ...   \n",
       "839594     -0.167367      TCP_Flood         0.007982             -0.013989   \n",
       "839595     -0.167367      TCP_Flood         0.007982             -0.013989   \n",
       "839596     -0.167367     ICMP_Flood         0.007982             -0.013989   \n",
       "839597     -0.167367      UDP_Flood         0.007982             -0.013989   \n",
       "839598     -0.167367      UDP_Flood         0.007982             -0.013989   \n",
       "\n",
       "        is_attack  \n",
       "0               1  \n",
       "1               1  \n",
       "2               1  \n",
       "3               1  \n",
       "4               1  \n",
       "...           ...  \n",
       "839594          1  \n",
       "839595          1  \n",
       "839596          1  \n",
       "839597          1  \n",
       "839598          1  \n",
       "\n",
       "[839599 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../../Datasets/Farm-Flow_Train.csv\")\n",
    "display(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missed_bytes</th>\n",
       "      <th>orig_pkts</th>\n",
       "      <th>orig_ip_bytes</th>\n",
       "      <th>resp_pkts</th>\n",
       "      <th>resp_ip_bytes</th>\n",
       "      <th>fwd_pkts_per_sec</th>\n",
       "      <th>bwd_pkts_per_sec</th>\n",
       "      <th>flow_pkts_per_sec</th>\n",
       "      <th>down_up_ratio</th>\n",
       "      <th>fwd_header_size_tot</th>\n",
       "      <th>fwd_header_size_min</th>\n",
       "      <th>fwd_header_size_max</th>\n",
       "      <th>bwd_header_size_tot</th>\n",
       "      <th>bwd_header_size_min</th>\n",
       "      <th>bwd_header_size_max</th>\n",
       "      <th>fwd_pkts_payload.tot</th>\n",
       "      <th>fwd_pkts_payload.avg</th>\n",
       "      <th>bwd_pkts_payload.tot</th>\n",
       "      <th>bwd_pkts_payload.avg</th>\n",
       "      <th>flow_pkts_payload.tot</th>\n",
       "      <th>flow_pkts_payload.avg</th>\n",
       "      <th>fwd_iat.tot</th>\n",
       "      <th>fwd_iat.avg</th>\n",
       "      <th>bwd_iat.tot</th>\n",
       "      <th>bwd_iat.avg</th>\n",
       "      <th>flow_iat.tot</th>\n",
       "      <th>flow_iat.avg</th>\n",
       "      <th>traffic</th>\n",
       "      <th>pkts_difference</th>\n",
       "      <th>data_pkts_difference</th>\n",
       "      <th>is_attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067841</td>\n",
       "      <td>1.692323</td>\n",
       "      <td>0.881829</td>\n",
       "      <td>1.503112</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>456</td>\n",
       "      <td>1.566169</td>\n",
       "      <td>2.350997</td>\n",
       "      <td>456</td>\n",
       "      <td>2.712843</td>\n",
       "      <td>3.183130</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>1.589631</td>\n",
       "      <td>3.342022</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>0.545786</td>\n",
       "      <td>-0.054924</td>\n",
       "      <td>0.848904</td>\n",
       "      <td>0.625086</td>\n",
       "      <td>0.541274</td>\n",
       "      <td>-0.112275</td>\n",
       "      <td>HTTP_Flood</td>\n",
       "      <td>-0.043444</td>\n",
       "      <td>0.446804</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067841</td>\n",
       "      <td>-0.276726</td>\n",
       "      <td>0.881829</td>\n",
       "      <td>1.189272</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.992063</td>\n",
       "      <td>0.992063</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>20</td>\n",
       "      <td>0.200850</td>\n",
       "      <td>0.168930</td>\n",
       "      <td>24</td>\n",
       "      <td>1.923476</td>\n",
       "      <td>1.736165</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>-0.052290</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.159412</td>\n",
       "      <td>-0.168388</td>\n",
       "      <td>-0.076153</td>\n",
       "      <td>-0.096974</td>\n",
       "      <td>-0.163659</td>\n",
       "      <td>-0.167355</td>\n",
       "      <td>TCP_Flood</td>\n",
       "      <td>-0.043444</td>\n",
       "      <td>-0.013989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067841</td>\n",
       "      <td>-0.276726</td>\n",
       "      <td>-0.500297</td>\n",
       "      <td>-0.536844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.200850</td>\n",
       "      <td>0.168930</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.444625</td>\n",
       "      <td>-0.434283</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>-0.052290</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.159412</td>\n",
       "      <td>-0.168388</td>\n",
       "      <td>-0.076153</td>\n",
       "      <td>-0.096974</td>\n",
       "      <td>-0.163665</td>\n",
       "      <td>-0.167367</td>\n",
       "      <td>TCP_Flood</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>-0.013989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067841</td>\n",
       "      <td>-0.276726</td>\n",
       "      <td>-0.500297</td>\n",
       "      <td>-0.536844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.200850</td>\n",
       "      <td>0.168930</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.444625</td>\n",
       "      <td>-0.434283</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>-0.052290</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.159412</td>\n",
       "      <td>-0.168388</td>\n",
       "      <td>-0.076153</td>\n",
       "      <td>-0.096974</td>\n",
       "      <td>-0.163665</td>\n",
       "      <td>-0.167367</td>\n",
       "      <td>TCP_Flood</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>-0.013989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067841</td>\n",
       "      <td>-0.393700</td>\n",
       "      <td>-0.500297</td>\n",
       "      <td>-0.536844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>-1.164469</td>\n",
       "      <td>-1.140311</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.444625</td>\n",
       "      <td>-0.434283</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>-0.052290</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.159412</td>\n",
       "      <td>-0.168388</td>\n",
       "      <td>-0.076153</td>\n",
       "      <td>-0.096974</td>\n",
       "      <td>-0.163665</td>\n",
       "      <td>-0.167367</td>\n",
       "      <td>UDP_Flood</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>-0.013989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279862</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067841</td>\n",
       "      <td>-0.237735</td>\n",
       "      <td>-0.500297</td>\n",
       "      <td>-0.536844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24</td>\n",
       "      <td>0.655956</td>\n",
       "      <td>0.605343</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.444625</td>\n",
       "      <td>-0.434283</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>-0.052290</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.159412</td>\n",
       "      <td>-0.168388</td>\n",
       "      <td>-0.076153</td>\n",
       "      <td>-0.096974</td>\n",
       "      <td>-0.163665</td>\n",
       "      <td>-0.167367</td>\n",
       "      <td>Port_Scanning</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>-0.013989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279863</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067841</td>\n",
       "      <td>-0.393700</td>\n",
       "      <td>-0.500297</td>\n",
       "      <td>-0.536844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>-1.164469</td>\n",
       "      <td>-1.140311</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.444625</td>\n",
       "      <td>-0.434283</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>-0.052290</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.159412</td>\n",
       "      <td>-0.168388</td>\n",
       "      <td>-0.076153</td>\n",
       "      <td>-0.096974</td>\n",
       "      <td>-0.163665</td>\n",
       "      <td>-0.167367</td>\n",
       "      <td>UDP_Flood</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>-0.013989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279864</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067841</td>\n",
       "      <td>-0.276726</td>\n",
       "      <td>-0.500297</td>\n",
       "      <td>-0.536844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.200850</td>\n",
       "      <td>0.168930</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.444625</td>\n",
       "      <td>-0.434283</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>-0.052290</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.159412</td>\n",
       "      <td>-0.168388</td>\n",
       "      <td>-0.076153</td>\n",
       "      <td>-0.096974</td>\n",
       "      <td>-0.163665</td>\n",
       "      <td>-0.167367</td>\n",
       "      <td>TCP_Flood</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>-0.013989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279865</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067841</td>\n",
       "      <td>-0.276726</td>\n",
       "      <td>-0.500297</td>\n",
       "      <td>-0.536844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.200850</td>\n",
       "      <td>0.168930</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.444625</td>\n",
       "      <td>-0.434283</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>-0.052290</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.159412</td>\n",
       "      <td>-0.168388</td>\n",
       "      <td>-0.076153</td>\n",
       "      <td>-0.096974</td>\n",
       "      <td>-0.163665</td>\n",
       "      <td>-0.167367</td>\n",
       "      <td>MQTT_Flood</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>-0.013989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279866</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.067841</td>\n",
       "      <td>-0.276726</td>\n",
       "      <td>-0.500297</td>\n",
       "      <td>-0.536844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.200850</td>\n",
       "      <td>0.168930</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.444625</td>\n",
       "      <td>-0.434283</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.019102</td>\n",
       "      <td>-0.052290</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.159412</td>\n",
       "      <td>-0.168388</td>\n",
       "      <td>-0.076153</td>\n",
       "      <td>-0.096974</td>\n",
       "      <td>-0.163665</td>\n",
       "      <td>-0.167367</td>\n",
       "      <td>TCP_Flood</td>\n",
       "      <td>0.007982</td>\n",
       "      <td>-0.013989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>279867 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        missed_bytes  orig_pkts  orig_ip_bytes  resp_pkts  resp_ip_bytes  \\\n",
       "0                0.0  -0.067841       1.692323   0.881829       1.503112   \n",
       "1                0.0  -0.067841      -0.276726   0.881829       1.189272   \n",
       "2                0.0  -0.067841      -0.276726  -0.500297      -0.536844   \n",
       "3                0.0  -0.067841      -0.276726  -0.500297      -0.536844   \n",
       "4                0.0  -0.067841      -0.393700  -0.500297      -0.536844   \n",
       "...              ...        ...            ...        ...            ...   \n",
       "279862           0.0  -0.067841      -0.237735  -0.500297      -0.536844   \n",
       "279863           0.0  -0.067841      -0.393700  -0.500297      -0.536844   \n",
       "279864           0.0  -0.067841      -0.276726  -0.500297      -0.536844   \n",
       "279865           0.0  -0.067841      -0.276726  -0.500297      -0.536844   \n",
       "279866           0.0  -0.067841      -0.276726  -0.500297      -0.536844   \n",
       "\n",
       "        fwd_pkts_per_sec  bwd_pkts_per_sec  flow_pkts_per_sec  down_up_ratio  \\\n",
       "0               0.000085          0.000116           0.000116       0.166667   \n",
       "1               0.730159          0.992063           0.992063       0.166667   \n",
       "2               0.000000          0.000000           0.000000       0.000000   \n",
       "3               0.000000          0.000000           0.000000       0.000000   \n",
       "4               0.000000          0.000000           0.000000       0.000000   \n",
       "...                  ...               ...                ...            ...   \n",
       "279862          0.000000          0.000000           0.000000       0.000000   \n",
       "279863          0.000000          0.000000           0.000000       0.000000   \n",
       "279864          0.000000          0.000000           0.000000       0.000000   \n",
       "279865          0.000000          0.000000           0.000000       0.000000   \n",
       "279866          0.000000          0.000000           0.000000       0.000000   \n",
       "\n",
       "        fwd_header_size_tot  fwd_header_size_min  fwd_header_size_max  \\\n",
       "0                       456             1.566169             2.350997   \n",
       "1                        20             0.200850             0.168930   \n",
       "2                        20             0.200850             0.168930   \n",
       "3                        20             0.200850             0.168930   \n",
       "4                         8            -1.164469            -1.140311   \n",
       "...                     ...                  ...                  ...   \n",
       "279862                   24             0.655956             0.605343   \n",
       "279863                    8            -1.164469            -1.140311   \n",
       "279864                   20             0.200850             0.168930   \n",
       "279865                   20             0.200850             0.168930   \n",
       "279866                   20             0.200850             0.168930   \n",
       "\n",
       "        bwd_header_size_tot  bwd_header_size_min  bwd_header_size_max  \\\n",
       "0                       456             2.712843             3.183130   \n",
       "1                        24             1.923476             1.736165   \n",
       "2                         0            -0.444625            -0.434283   \n",
       "3                         0            -0.444625            -0.434283   \n",
       "4                         0            -0.444625            -0.434283   \n",
       "...                     ...                  ...                  ...   \n",
       "279862                    0            -0.444625            -0.434283   \n",
       "279863                    0            -0.444625            -0.434283   \n",
       "279864                    0            -0.444625            -0.434283   \n",
       "279865                    0            -0.444625            -0.434283   \n",
       "279866                    0            -0.444625            -0.434283   \n",
       "\n",
       "        fwd_pkts_payload.tot  fwd_pkts_payload.avg  bwd_pkts_payload.tot  \\\n",
       "0                  -0.000945             -0.000945              1.589631   \n",
       "1                  -0.000945             -0.000945             -0.019102   \n",
       "2                  -0.000945             -0.000945             -0.019102   \n",
       "3                  -0.000945             -0.000945             -0.019102   \n",
       "4                  -0.000945             -0.000945             -0.019102   \n",
       "...                      ...                   ...                   ...   \n",
       "279862             -0.000945             -0.000945             -0.019102   \n",
       "279863             -0.000945             -0.000945             -0.019102   \n",
       "279864             -0.000945             -0.000945             -0.019102   \n",
       "279865             -0.000945             -0.000945             -0.019102   \n",
       "279866             -0.000945             -0.000945             -0.019102   \n",
       "\n",
       "        bwd_pkts_payload.avg  flow_pkts_payload.tot  flow_pkts_payload.avg  \\\n",
       "0                   3.342022              -0.000945              -0.000945   \n",
       "1                  -0.052290              -0.000945              -0.000945   \n",
       "2                  -0.052290              -0.000945              -0.000945   \n",
       "3                  -0.052290              -0.000945              -0.000945   \n",
       "4                  -0.052290              -0.000945              -0.000945   \n",
       "...                      ...                    ...                    ...   \n",
       "279862             -0.052290              -0.000945              -0.000945   \n",
       "279863             -0.052290              -0.000945              -0.000945   \n",
       "279864             -0.052290              -0.000945              -0.000945   \n",
       "279865             -0.052290              -0.000945              -0.000945   \n",
       "279866             -0.052290              -0.000945              -0.000945   \n",
       "\n",
       "        fwd_iat.tot  fwd_iat.avg  bwd_iat.tot  bwd_iat.avg  flow_iat.tot  \\\n",
       "0          0.545786    -0.054924     0.848904     0.625086      0.541274   \n",
       "1         -0.159412    -0.168388    -0.076153    -0.096974     -0.163659   \n",
       "2         -0.159412    -0.168388    -0.076153    -0.096974     -0.163665   \n",
       "3         -0.159412    -0.168388    -0.076153    -0.096974     -0.163665   \n",
       "4         -0.159412    -0.168388    -0.076153    -0.096974     -0.163665   \n",
       "...             ...          ...          ...          ...           ...   \n",
       "279862    -0.159412    -0.168388    -0.076153    -0.096974     -0.163665   \n",
       "279863    -0.159412    -0.168388    -0.076153    -0.096974     -0.163665   \n",
       "279864    -0.159412    -0.168388    -0.076153    -0.096974     -0.163665   \n",
       "279865    -0.159412    -0.168388    -0.076153    -0.096974     -0.163665   \n",
       "279866    -0.159412    -0.168388    -0.076153    -0.096974     -0.163665   \n",
       "\n",
       "        flow_iat.avg        traffic  pkts_difference  data_pkts_difference  \\\n",
       "0          -0.112275     HTTP_Flood        -0.043444              0.446804   \n",
       "1          -0.167355      TCP_Flood        -0.043444             -0.013989   \n",
       "2          -0.167367      TCP_Flood         0.007982             -0.013989   \n",
       "3          -0.167367      TCP_Flood         0.007982             -0.013989   \n",
       "4          -0.167367      UDP_Flood         0.007982             -0.013989   \n",
       "...              ...            ...              ...                   ...   \n",
       "279862     -0.167367  Port_Scanning         0.007982             -0.013989   \n",
       "279863     -0.167367      UDP_Flood         0.007982             -0.013989   \n",
       "279864     -0.167367      TCP_Flood         0.007982             -0.013989   \n",
       "279865     -0.167367     MQTT_Flood         0.007982             -0.013989   \n",
       "279866     -0.167367      TCP_Flood         0.007982             -0.013989   \n",
       "\n",
       "        is_attack  \n",
       "0               1  \n",
       "1               1  \n",
       "2               1  \n",
       "3               1  \n",
       "4               1  \n",
       "...           ...  \n",
       "279862          1  \n",
       "279863          1  \n",
       "279864          1  \n",
       "279865          1  \n",
       "279866          1  \n",
       "\n",
       "[279867 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"../../Datasets/Farm-Flow_Test.csv\")\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "traffic\n",
       "Arp_Spoofing       2007\n",
       "BotNet_DDOS          35\n",
       "HTTP_Flood        84549\n",
       "ICMP_Flood        57569\n",
       "MQTT_Flood        93349\n",
       "Normal             5321\n",
       "Port_Scanning     94909\n",
       "TCP_Flood        279623\n",
       "UDP_Flood        222237\n",
       "Name: traffic, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby('traffic')['traffic'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "traffic\n",
       "Arp_Spoofing       685\n",
       "BotNet_DDOS         13\n",
       "HTTP_Flood       28412\n",
       "ICMP_Flood       19246\n",
       "MQTT_Flood       30639\n",
       "Normal            1774\n",
       "Port_Scanning    31664\n",
       "TCP_Flood        93123\n",
       "UDP_Flood        74311\n",
       "Name: traffic, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.groupby('traffic')['traffic'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcHY3Kk_N_XN"
   },
   "source": [
    "---------------------------------------\n",
    "\n",
    "**Create Model & Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop('is_attack', axis=1)\n",
    "df_test = df_test.drop('is_attack', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "g7ExpPs6N_XN"
   },
   "outputs": [],
   "source": [
    "x_columns = df_train.columns.drop('traffic')\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df_train[\"traffic\"].values)\n",
    "\n",
    "x = df_train[x_columns].values\n",
    "y = df_train[\"traffic\"].values\n",
    "y = le.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_columns_val = df_test.columns.drop('traffic')\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df_test[\"traffic\"].values)\n",
    "\n",
    "x_val = df_test[x_columns].values\n",
    "y_val = df_test[\"traffic\"].values\n",
    "y_val = le.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((209900, 29), (209900,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((629699, 29), (629699,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((279867, 29), (279867,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_za--boceRr9"
   },
   "outputs": [],
   "source": [
    "# Usage of ExtraTreesClassifier for feature selection\n",
    "extra_tree_forest = ExtraTreesClassifier(n_estimators = 5, criterion ='entropy', max_features = 2)\n",
    "extra_tree_forest.fit(x, y)\n",
    "feature_importance = extra_tree_forest.feature_importances_\n",
    "feature_importance_normalized = np.std([tree.feature_importances_ for tree in  extra_tree_forest.estimators_], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "kBfWThvfeTNB",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for the ExtraTreesClassifier output\n",
    "plot.bar(x_columns, feature_importance_normalized)\n",
    "plot.xlabel('Feature Labels')\n",
    "plot.ylabel('Feature Importances')\n",
    "plot.title('Comparison of different feature importances in the current dataset')\n",
    "plot.xticks(rotation = 90)\n",
    "\n",
    "# Plot size\n",
    "plot.rcParams[\"figure.figsize\"] = (70, 40)\n",
    "\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kmWyRAR0N_XN",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 629699 samples, validate on 209900 samples\n",
      "Epoch 1/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 1.2438 - accuracy: 0.6469 - val_loss: 0.8175 - val_accuracy: 0.7258\n",
      "Epoch 2/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.8003 - accuracy: 0.7645 - val_loss: 0.9311 - val_accuracy: 0.7279\n",
      "Epoch 3/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.6974 - accuracy: 0.7904 - val_loss: 0.7283 - val_accuracy: 0.7373\n",
      "Epoch 4/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.6723 - accuracy: 0.7953 - val_loss: 0.6524 - val_accuracy: 0.7384\n",
      "Epoch 5/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.6413 - accuracy: 0.8028 - val_loss: 0.5240 - val_accuracy: 0.8254\n",
      "Epoch 6/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.6227 - accuracy: 0.8067 - val_loss: 0.5745 - val_accuracy: 0.8469\n",
      "Epoch 7/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.6122 - accuracy: 0.8090 - val_loss: 0.5087 - val_accuracy: 0.8363\n",
      "Epoch 8/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.6174 - accuracy: 0.8075 - val_loss: 0.8705 - val_accuracy: 0.7458\n",
      "Epoch 9/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.6112 - accuracy: 0.8080 - val_loss: 0.6535 - val_accuracy: 0.7406\n",
      "Epoch 10/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.6012 - accuracy: 0.8098 - val_loss: 0.6200 - val_accuracy: 0.7463\n",
      "Epoch 11/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.5949 - accuracy: 0.8113 - val_loss: 0.4970 - val_accuracy: 0.8324\n",
      "Epoch 12/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.5844 - accuracy: 0.8152 - val_loss: 0.5051 - val_accuracy: 0.8320\n",
      "Epoch 13/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.5918 - accuracy: 0.8127 - val_loss: 0.4918 - val_accuracy: 0.8330\n",
      "Epoch 14/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.5897 - accuracy: 0.8129 - val_loss: 0.5229 - val_accuracy: 0.8194\n",
      "Epoch 15/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.5751 - accuracy: 0.8164 - val_loss: 0.4932 - val_accuracy: 0.8447\n",
      "Epoch 16/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.5751 - accuracy: 0.8163 - val_loss: 0.5881 - val_accuracy: 0.7579\n",
      "Epoch 17/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.5739 - accuracy: 0.8167 - val_loss: 1.0948 - val_accuracy: 0.7475\n",
      "Epoch 18/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.5824 - accuracy: 0.8142 - val_loss: 0.5726 - val_accuracy: 0.8213\n",
      "Epoch 19/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.6019 - accuracy: 0.8107 - val_loss: 0.4826 - val_accuracy: 0.8447\n",
      "Epoch 20/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.6003 - accuracy: 0.8100 - val_loss: 1.3148 - val_accuracy: 0.7581\n",
      "Epoch 21/300\n",
      "629699/629699 [==============================] - 10s 16us/sample - loss: 0.6182 - accuracy: 0.8055 - val_loss: 0.5373 - val_accuracy: 0.8457\n",
      "Epoch 22/300\n",
      "629699/629699 [==============================] - 18s 29us/sample - loss: 0.6192 - accuracy: 0.8054 - val_loss: 0.4996 - val_accuracy: 0.8479\n",
      "Epoch 23/300\n",
      "629699/629699 [==============================] - 19s 29us/sample - loss: 0.6573 - accuracy: 0.7946 - val_loss: 1.1219 - val_accuracy: 0.7375\n",
      "Epoch 24/300\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.6647 - accuracy: 0.7941 - val_loss: 0.5302 - val_accuracy: 0.8287\n",
      "Epoch 25/300\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.6632 - accuracy: 0.7938 - val_loss: 0.6346 - val_accuracy: 0.7519\n",
      "Epoch 26/300\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.6728 - accuracy: 0.7911 - val_loss: 0.4945 - val_accuracy: 0.8459\n",
      "Epoch 27/300\n",
      "629248/629699 [============================>.] - ETA: 0s - loss: 0.6934 - accuracy: 0.7841\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "629699/629699 [==============================] - 17s 28us/sample - loss: 0.6935 - accuracy: 0.7840 - val_loss: 0.5443 - val_accuracy: 0.8254\n",
      "Epoch 28/300\n",
      "629699/629699 [==============================] - 17s 28us/sample - loss: 0.6952 - accuracy: 0.7830 - val_loss: 0.7751 - val_accuracy: 0.7551\n",
      "Epoch 29/300\n",
      "629699/629699 [==============================] - 17s 28us/sample - loss: 0.6698 - accuracy: 0.7896 - val_loss: 0.5050 - val_accuracy: 0.8409\n",
      "Epoch 30/300\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.6734 - accuracy: 0.7875 - val_loss: 0.4916 - val_accuracy: 0.8434\n",
      "Epoch 31/300\n",
      "629699/629699 [==============================] - 7s 11us/sample - loss: 0.6589 - accuracy: 0.7912 - val_loss: 0.5031 - val_accuracy: 0.8419\n",
      "Epoch 32/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.6807 - accuracy: 0.7863 - val_loss: 0.5171 - val_accuracy: 0.8454\n",
      "Epoch 33/300\n",
      "629699/629699 [==============================] - 3s 6us/sample - loss: 0.7532 - accuracy: 0.7650 - val_loss: 0.5281 - val_accuracy: 0.8367\n",
      "Epoch 34/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7215 - accuracy: 0.7747 - val_loss: 0.9338 - val_accuracy: 0.7514\n",
      "Epoch 35/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7681 - accuracy: 0.7610 - val_loss: 0.5172 - val_accuracy: 0.8267\n",
      "Epoch 36/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7523 - accuracy: 0.7655 - val_loss: 0.5279 - val_accuracy: 0.8284\n",
      "Epoch 37/300\n",
      "626688/629699 [============================>.] - ETA: 0s - loss: 0.8491 - accuracy: 0.7325\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.8491 - accuracy: 0.7325 - val_loss: 0.5660 - val_accuracy: 0.8274\n",
      "Epoch 38/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.8515 - accuracy: 0.7252 - val_loss: 0.5109 - val_accuracy: 0.8326\n",
      "Epoch 39/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.8152 - accuracy: 0.7401 - val_loss: 0.5403 - val_accuracy: 0.8437\n",
      "Epoch 40/300\n",
      "629699/629699 [==============================] - 7s 11us/sample - loss: 0.8031 - accuracy: 0.7456 - val_loss: 0.5381 - val_accuracy: 0.8300\n",
      "Epoch 41/300\n",
      "629699/629699 [==============================] - 17s 28us/sample - loss: 0.7827 - accuracy: 0.7530 - val_loss: 0.5174 - val_accuracy: 0.8314\n",
      "Epoch 42/300\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.7523 - accuracy: 0.7617 - val_loss: 0.4999 - val_accuracy: 0.8311\n",
      "Epoch 43/300\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.7658 - accuracy: 0.7577 - val_loss: 0.5156 - val_accuracy: 0.8190\n",
      "Epoch 44/300\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.7453 - accuracy: 0.7628 - val_loss: 0.4948 - val_accuracy: 0.8300\n",
      "Epoch 45/300\n",
      "629699/629699 [==============================] - 12s 19us/sample - loss: 0.7437 - accuracy: 0.7635 - val_loss: 0.4945 - val_accuracy: 0.8294\n",
      "Epoch 46/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7431 - accuracy: 0.7626 - val_loss: 0.5076 - val_accuracy: 0.8208\n",
      "Epoch 47/300\n",
      "628224/629699 [============================>.] - ETA: 0s - loss: 0.8113 - accuracy: 0.7452\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.8111 - accuracy: 0.7453 - val_loss: 0.5473 - val_accuracy: 0.8305\n",
      "Epoch 48/300\n",
      "629699/629699 [==============================] - 3s 6us/sample - loss: 0.7400 - accuracy: 0.7649 - val_loss: 0.5032 - val_accuracy: 0.8428\n",
      "Epoch 49/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7385 - accuracy: 0.7645 - val_loss: 0.4897 - val_accuracy: 0.8312\n",
      "Epoch 50/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7349 - accuracy: 0.7651 - val_loss: 0.5012 - val_accuracy: 0.8313\n",
      "Epoch 51/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7322 - accuracy: 0.7662 - val_loss: 0.4978 - val_accuracy: 0.8313\n",
      "Epoch 52/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7343 - accuracy: 0.7648 - val_loss: 0.4953 - val_accuracy: 0.8313\n",
      "Epoch 53/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7327 - accuracy: 0.7651 - val_loss: 0.4992 - val_accuracy: 0.8313\n",
      "Epoch 54/300\n",
      "629699/629699 [==============================] - 7s 11us/sample - loss: 0.7300 - accuracy: 0.7661 - val_loss: 0.4882 - val_accuracy: 0.8312\n",
      "Epoch 55/300\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.7339 - accuracy: 0.7647 - val_loss: 0.4939 - val_accuracy: 0.8423\n",
      "Epoch 56/300\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.7701 - accuracy: 0.7547 - val_loss: 0.5038 - val_accuracy: 0.8469\n",
      "Epoch 57/300\n",
      "629248/629699 [============================>.] - ETA: 0s - loss: 0.7470 - accuracy: 0.7608\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.7469 - accuracy: 0.7608 - val_loss: 0.4903 - val_accuracy: 0.8313\n",
      "Epoch 58/300\n",
      "629699/629699 [==============================] - 17s 28us/sample - loss: 0.7281 - accuracy: 0.7668 - val_loss: 0.4891 - val_accuracy: 0.8426\n",
      "Epoch 59/300\n",
      "629699/629699 [==============================] - 14s 22us/sample - loss: 0.7277 - accuracy: 0.7660 - val_loss: 0.5002 - val_accuracy: 0.8315\n",
      "Epoch 60/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7287 - accuracy: 0.7663 - val_loss: 0.4843 - val_accuracy: 0.8425\n",
      "Epoch 61/300\n",
      "629699/629699 [==============================] - 3s 6us/sample - loss: 0.7285 - accuracy: 0.7664 - val_loss: 0.4857 - val_accuracy: 0.8426\n",
      "Epoch 62/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7268 - accuracy: 0.7669 - val_loss: 0.4872 - val_accuracy: 0.8424\n",
      "Epoch 63/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7270 - accuracy: 0.7667 - val_loss: 0.4829 - val_accuracy: 0.8426\n",
      "Epoch 64/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7262 - accuracy: 0.7668 - val_loss: 0.4978 - val_accuracy: 0.8312\n",
      "Epoch 65/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7262 - accuracy: 0.7667 - val_loss: 0.4930 - val_accuracy: 0.8313\n",
      "Epoch 66/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7264 - accuracy: 0.7666 - val_loss: 0.4867 - val_accuracy: 0.8313\n",
      "Epoch 67/300\n",
      "629248/629699 [============================>.] - ETA: 0s - loss: 0.7248 - accuracy: 0.7673\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7247 - accuracy: 0.7674 - val_loss: 0.4852 - val_accuracy: 0.8425\n",
      "Epoch 68/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7249 - accuracy: 0.7669 - val_loss: 0.4837 - val_accuracy: 0.8407\n",
      "Epoch 69/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7247 - accuracy: 0.7668 - val_loss: 0.4840 - val_accuracy: 0.8415\n",
      "Epoch 70/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7250 - accuracy: 0.7674 - val_loss: 0.5081 - val_accuracy: 0.8313\n",
      "Epoch 71/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7250 - accuracy: 0.7669 - val_loss: 0.5100 - val_accuracy: 0.8314\n",
      "Epoch 72/300\n",
      "629699/629699 [==============================] - 10s 17us/sample - loss: 0.7255 - accuracy: 0.7668 - val_loss: 0.4864 - val_accuracy: 0.8426\n",
      "Epoch 73/300\n",
      "629699/629699 [==============================] - 17s 28us/sample - loss: 0.7240 - accuracy: 0.7675 - val_loss: 0.4977 - val_accuracy: 0.8318\n",
      "Epoch 74/300\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.7225 - accuracy: 0.7678 - val_loss: 0.4897 - val_accuracy: 0.8424\n",
      "Epoch 75/300\n",
      "629699/629699 [==============================] - 17s 28us/sample - loss: 0.7244 - accuracy: 0.7675 - val_loss: 0.4846 - val_accuracy: 0.8426\n",
      "Epoch 76/300\n",
      "629699/629699 [==============================] - 4s 7us/sample - loss: 0.7237 - accuracy: 0.7672 - val_loss: 0.5108 - val_accuracy: 0.8313\n",
      "Epoch 77/300\n",
      "621056/629699 [============================>.] - ETA: 0s - loss: 0.7239 - accuracy: 0.7676\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7238 - accuracy: 0.7677 - val_loss: 0.4981 - val_accuracy: 0.8302\n",
      "Epoch 78/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7232 - accuracy: 0.7676 - val_loss: 0.5004 - val_accuracy: 0.8313\n",
      "Epoch 79/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7230 - accuracy: 0.7677 - val_loss: 0.4881 - val_accuracy: 0.8428\n",
      "Epoch 80/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7215 - accuracy: 0.7680 - val_loss: 0.4824 - val_accuracy: 0.8426\n",
      "Epoch 81/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7211 - accuracy: 0.7682 - val_loss: 0.4911 - val_accuracy: 0.8432\n",
      "Epoch 82/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7227 - accuracy: 0.7677 - val_loss: 0.4828 - val_accuracy: 0.8427\n",
      "Epoch 83/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7234 - accuracy: 0.7675 - val_loss: 0.4960 - val_accuracy: 0.8425\n",
      "Epoch 84/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7224 - accuracy: 0.7676 - val_loss: 0.4819 - val_accuracy: 0.8425\n",
      "Epoch 85/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7224 - accuracy: 0.7675 - val_loss: 0.5076 - val_accuracy: 0.8313\n",
      "Epoch 86/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7235 - accuracy: 0.7672 - val_loss: 0.4918 - val_accuracy: 0.8428\n",
      "Epoch 87/300\n",
      "620032/629699 [============================>.] - ETA: 0s - loss: 0.7228 - accuracy: 0.7671\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7227 - accuracy: 0.7671 - val_loss: 0.4982 - val_accuracy: 0.8426\n",
      "Epoch 88/300\n",
      "629699/629699 [==============================] - 8s 12us/sample - loss: 0.7217 - accuracy: 0.7677 - val_loss: 0.4919 - val_accuracy: 0.8428\n",
      "Epoch 89/300\n",
      "629699/629699 [==============================] - 17s 28us/sample - loss: 0.7235 - accuracy: 0.7673 - val_loss: 0.4789 - val_accuracy: 0.8427\n",
      "Epoch 90/300\n",
      "629699/629699 [==============================] - 17s 28us/sample - loss: 0.7216 - accuracy: 0.7683 - val_loss: 0.4841 - val_accuracy: 0.8426\n",
      "Epoch 91/300\n",
      "629699/629699 [==============================] - 17s 28us/sample - loss: 0.7214 - accuracy: 0.7676 - val_loss: 0.4843 - val_accuracy: 0.8428\n",
      "Epoch 92/300\n",
      "629699/629699 [==============================] - 6s 10us/sample - loss: 0.7212 - accuracy: 0.7679 - val_loss: 0.4882 - val_accuracy: 0.8428\n",
      "Epoch 93/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7219 - accuracy: 0.7672 - val_loss: 0.4877 - val_accuracy: 0.8425\n",
      "Epoch 94/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7227 - accuracy: 0.7670 - val_loss: 0.4843 - val_accuracy: 0.8425\n",
      "Epoch 95/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7227 - accuracy: 0.7676 - val_loss: 0.5036 - val_accuracy: 0.8428\n",
      "Epoch 96/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7227 - accuracy: 0.7669 - val_loss: 0.5142 - val_accuracy: 0.8312\n",
      "Epoch 97/300\n",
      "622080/629699 [============================>.] - ETA: 0s - loss: 0.7228 - accuracy: 0.7674\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7226 - accuracy: 0.7675 - val_loss: 0.4955 - val_accuracy: 0.8431\n",
      "Epoch 98/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7222 - accuracy: 0.7677 - val_loss: 0.4860 - val_accuracy: 0.8425\n",
      "Epoch 99/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7223 - accuracy: 0.7674 - val_loss: 0.4932 - val_accuracy: 0.8425\n",
      "Epoch 100/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7223 - accuracy: 0.7675 - val_loss: 0.4972 - val_accuracy: 0.8425\n",
      "Epoch 101/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7227 - accuracy: 0.7670 - val_loss: 0.4843 - val_accuracy: 0.8433\n",
      "Epoch 102/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7226 - accuracy: 0.7677 - val_loss: 0.5053 - val_accuracy: 0.8313\n",
      "Epoch 103/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7226 - accuracy: 0.7674 - val_loss: 0.5104 - val_accuracy: 0.8425\n",
      "Epoch 104/300\n",
      "629699/629699 [==============================] - 3s 6us/sample - loss: 0.7225 - accuracy: 0.7676 - val_loss: 0.5123 - val_accuracy: 0.8320\n",
      "Epoch 105/300\n",
      "629699/629699 [==============================] - 9s 15us/sample - loss: 0.7217 - accuracy: 0.7680 - val_loss: 0.4876 - val_accuracy: 0.8319\n",
      "Epoch 106/300\n",
      "629699/629699 [==============================] - 17s 28us/sample - loss: 0.7211 - accuracy: 0.7680 - val_loss: 0.4918 - val_accuracy: 0.8425\n",
      "Epoch 107/300\n",
      "627712/629699 [============================>.] - ETA: 0s - loss: 0.7207 - accuracy: 0.7682\n",
      "Epoch 00107: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "629699/629699 [==============================] - 17s 28us/sample - loss: 0.7206 - accuracy: 0.7683 - val_loss: 0.5082 - val_accuracy: 0.8321\n",
      "Epoch 108/300\n",
      "629699/629699 [==============================] - 16s 26us/sample - loss: 0.7207 - accuracy: 0.7682 - val_loss: 0.4827 - val_accuracy: 0.8426\n",
      "Epoch 109/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7201 - accuracy: 0.7684 - val_loss: 0.4788 - val_accuracy: 0.8425\n",
      "Epoch 110/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7231 - accuracy: 0.7678 - val_loss: 0.4809 - val_accuracy: 0.8433\n",
      "Epoch 111/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7209 - accuracy: 0.7680 - val_loss: 0.4803 - val_accuracy: 0.8425\n",
      "Epoch 112/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7215 - accuracy: 0.7679 - val_loss: 0.4805 - val_accuracy: 0.8426\n",
      "Epoch 113/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7212 - accuracy: 0.7680 - val_loss: 0.4955 - val_accuracy: 0.8320\n",
      "Epoch 114/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7212 - accuracy: 0.7680 - val_loss: 0.4935 - val_accuracy: 0.8425\n",
      "Epoch 115/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7228 - accuracy: 0.7678 - val_loss: 0.4911 - val_accuracy: 0.8428\n",
      "Epoch 116/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7219 - accuracy: 0.7679 - val_loss: 0.4865 - val_accuracy: 0.8427\n",
      "Epoch 117/300\n",
      "619008/629699 [============================>.] - ETA: 0s - loss: 0.7223 - accuracy: 0.7676\n",
      "Epoch 00117: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7221 - accuracy: 0.7677 - val_loss: 0.5010 - val_accuracy: 0.8319\n",
      "Epoch 118/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7206 - accuracy: 0.7680 - val_loss: 0.4943 - val_accuracy: 0.8321\n",
      "Epoch 119/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7212 - accuracy: 0.7682 - val_loss: 0.4948 - val_accuracy: 0.8313\n",
      "Epoch 120/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7224 - accuracy: 0.7675 - val_loss: 0.4867 - val_accuracy: 0.8429\n",
      "Epoch 121/300\n",
      "629699/629699 [==============================] - 11s 18us/sample - loss: 0.7205 - accuracy: 0.7679 - val_loss: 0.5089 - val_accuracy: 0.8319\n",
      "Epoch 122/300\n",
      "629699/629699 [==============================] - 17s 28us/sample - loss: 0.7210 - accuracy: 0.7679 - val_loss: 0.4971 - val_accuracy: 0.8318\n",
      "Epoch 123/300\n",
      "629699/629699 [==============================] - 17s 28us/sample - loss: 0.7206 - accuracy: 0.7680 - val_loss: 0.4847 - val_accuracy: 0.8428\n",
      "Epoch 124/300\n",
      "629699/629699 [==============================] - 15s 24us/sample - loss: 0.7213 - accuracy: 0.7678 - val_loss: 0.4806 - val_accuracy: 0.8428\n",
      "Epoch 125/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7213 - accuracy: 0.7679 - val_loss: 0.4837 - val_accuracy: 0.8429\n",
      "Epoch 126/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7205 - accuracy: 0.7681 - val_loss: 0.4811 - val_accuracy: 0.8429\n",
      "Epoch 127/300\n",
      "625152/629699 [============================>.] - ETA: 0s - loss: 0.7214 - accuracy: 0.7681\n",
      "Epoch 00127: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7216 - accuracy: 0.7681 - val_loss: 0.4861 - val_accuracy: 0.8425\n",
      "Epoch 128/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7210 - accuracy: 0.7687 - val_loss: 0.4862 - val_accuracy: 0.8425\n",
      "Epoch 129/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7228 - accuracy: 0.7676 - val_loss: 0.4834 - val_accuracy: 0.8426\n",
      "Epoch 130/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7205 - accuracy: 0.7684 - val_loss: 0.4973 - val_accuracy: 0.8321\n",
      "Epoch 131/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7223 - accuracy: 0.7681 - val_loss: 0.5127 - val_accuracy: 0.8314\n",
      "Epoch 132/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7219 - accuracy: 0.7675 - val_loss: 0.5035 - val_accuracy: 0.8321\n",
      "Epoch 133/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7214 - accuracy: 0.7681 - val_loss: 0.4804 - val_accuracy: 0.8426\n",
      "Epoch 134/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7215 - accuracy: 0.7678 - val_loss: 0.4837 - val_accuracy: 0.8425\n",
      "Epoch 135/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7212 - accuracy: 0.7676 - val_loss: 0.4827 - val_accuracy: 0.8426\n",
      "Epoch 136/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7227 - accuracy: 0.7676 - val_loss: 0.4908 - val_accuracy: 0.8426\n",
      "Epoch 137/300\n",
      "623616/629699 [============================>.] - ETA: 0s - loss: 0.7214 - accuracy: 0.7678\n",
      "Epoch 00137: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7215 - accuracy: 0.7679 - val_loss: 0.4946 - val_accuracy: 0.8426\n",
      "Epoch 138/300\n",
      "629699/629699 [==============================] - 7s 11us/sample - loss: 0.7216 - accuracy: 0.7675 - val_loss: 0.4915 - val_accuracy: 0.8319\n",
      "Epoch 139/300\n",
      "629699/629699 [==============================] - 17s 28us/sample - loss: 0.7216 - accuracy: 0.7678 - val_loss: 0.4907 - val_accuracy: 0.8316\n",
      "Epoch 140/300\n",
      "629699/629699 [==============================] - 17s 28us/sample - loss: 0.7203 - accuracy: 0.7679 - val_loss: 0.5065 - val_accuracy: 0.8313\n",
      "Epoch 141/300\n",
      "629699/629699 [==============================] - 17s 27us/sample - loss: 0.7200 - accuracy: 0.7679 - val_loss: 0.4803 - val_accuracy: 0.8426\n",
      "Epoch 142/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7202 - accuracy: 0.7682 - val_loss: 0.4966 - val_accuracy: 0.8320\n",
      "Epoch 143/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7221 - accuracy: 0.7674 - val_loss: 0.4816 - val_accuracy: 0.8431\n",
      "Epoch 144/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7224 - accuracy: 0.7678 - val_loss: 0.4991 - val_accuracy: 0.8313\n",
      "Epoch 145/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7197 - accuracy: 0.7680 - val_loss: 0.4827 - val_accuracy: 0.8425\n",
      "Epoch 146/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7221 - accuracy: 0.7677 - val_loss: 0.4796 - val_accuracy: 0.8433\n",
      "Epoch 147/300\n",
      "617984/629699 [============================>.] - ETA: 0s - loss: 0.7199 - accuracy: 0.7687\n",
      "Epoch 00147: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7202 - accuracy: 0.7687 - val_loss: 0.4991 - val_accuracy: 0.8313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7227 - accuracy: 0.7673 - val_loss: 0.4841 - val_accuracy: 0.8429\n",
      "Epoch 149/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7207 - accuracy: 0.7682 - val_loss: 0.4907 - val_accuracy: 0.8424\n",
      "Epoch 150/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7214 - accuracy: 0.7680 - val_loss: 0.4999 - val_accuracy: 0.8314\n",
      "Epoch 151/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7210 - accuracy: 0.7679 - val_loss: 0.5080 - val_accuracy: 0.8313\n",
      "Epoch 152/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7209 - accuracy: 0.7686 - val_loss: 0.4935 - val_accuracy: 0.8425\n",
      "Epoch 153/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7219 - accuracy: 0.7678 - val_loss: 0.4874 - val_accuracy: 0.8424\n",
      "Epoch 154/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7224 - accuracy: 0.7680 - val_loss: 0.5029 - val_accuracy: 0.8428\n",
      "Epoch 155/300\n",
      "629699/629699 [==============================] - 604s 959us/sample - loss: 0.7210 - accuracy: 0.7683 - val_loss: 0.5023 - val_accuracy: 0.8315\n",
      "Epoch 156/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7210 - accuracy: 0.7683 - val_loss: 0.4946 - val_accuracy: 0.8427\n",
      "Epoch 157/300\n",
      "625664/629699 [============================>.] - ETA: 0s - loss: 0.7220 - accuracy: 0.7675\n",
      "Epoch 00157: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7219 - accuracy: 0.7676 - val_loss: 0.4937 - val_accuracy: 0.8427\n",
      "Epoch 158/300\n",
      "629699/629699 [==============================] - 3s 6us/sample - loss: 0.7222 - accuracy: 0.7674 - val_loss: 0.4911 - val_accuracy: 0.8426\n",
      "Epoch 159/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7195 - accuracy: 0.7684 - val_loss: 0.4931 - val_accuracy: 0.8425\n",
      "Epoch 160/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7217 - accuracy: 0.7677 - val_loss: 0.4791 - val_accuracy: 0.8431\n",
      "Epoch 161/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7208 - accuracy: 0.7681 - val_loss: 0.4873 - val_accuracy: 0.8425\n",
      "Epoch 162/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7213 - accuracy: 0.7682 - val_loss: 0.4983 - val_accuracy: 0.8312\n",
      "Epoch 163/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7213 - accuracy: 0.7677 - val_loss: 0.4934 - val_accuracy: 0.8426\n",
      "Epoch 164/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7211 - accuracy: 0.7683 - val_loss: 0.4862 - val_accuracy: 0.8428\n",
      "Epoch 165/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7220 - accuracy: 0.7680 - val_loss: 0.4975 - val_accuracy: 0.8425\n",
      "Epoch 166/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7208 - accuracy: 0.7680 - val_loss: 0.4929 - val_accuracy: 0.8425\n",
      "Epoch 167/300\n",
      "622592/629699 [============================>.] - ETA: 0s - loss: 0.7211 - accuracy: 0.7676\n",
      "Epoch 00167: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7212 - accuracy: 0.7676 - val_loss: 0.4800 - val_accuracy: 0.8425\n",
      "Epoch 168/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7213 - accuracy: 0.7678 - val_loss: 0.4798 - val_accuracy: 0.8427\n",
      "Epoch 169/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7217 - accuracy: 0.7680 - val_loss: 0.4928 - val_accuracy: 0.8432\n",
      "Epoch 170/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7212 - accuracy: 0.7680 - val_loss: 0.4917 - val_accuracy: 0.8426\n",
      "Epoch 171/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7208 - accuracy: 0.7682 - val_loss: 0.4871 - val_accuracy: 0.8425\n",
      "Epoch 172/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7214 - accuracy: 0.7680 - val_loss: 0.4816 - val_accuracy: 0.8432\n",
      "Epoch 173/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7212 - accuracy: 0.7681 - val_loss: 0.4974 - val_accuracy: 0.8425\n",
      "Epoch 174/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7219 - accuracy: 0.7678 - val_loss: 0.4949 - val_accuracy: 0.8426\n",
      "Epoch 175/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7217 - accuracy: 0.7679 - val_loss: 0.5056 - val_accuracy: 0.8418\n",
      "Epoch 176/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7219 - accuracy: 0.7679 - val_loss: 0.4955 - val_accuracy: 0.8426\n",
      "Epoch 177/300\n",
      "626176/629699 [============================>.] - ETA: 0s - loss: 0.7212 - accuracy: 0.7683\n",
      "Epoch 00177: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7211 - accuracy: 0.7684 - val_loss: 0.5043 - val_accuracy: 0.8313\n",
      "Epoch 178/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7213 - accuracy: 0.7677 - val_loss: 0.4917 - val_accuracy: 0.8321\n",
      "Epoch 179/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7226 - accuracy: 0.7674 - val_loss: 0.4850 - val_accuracy: 0.8424\n",
      "Epoch 180/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7212 - accuracy: 0.7681 - val_loss: 0.4779 - val_accuracy: 0.8433\n",
      "Epoch 181/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7209 - accuracy: 0.7685 - val_loss: 0.5078 - val_accuracy: 0.8316\n",
      "Epoch 182/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7232 - accuracy: 0.7672 - val_loss: 0.4932 - val_accuracy: 0.8426\n",
      "Epoch 183/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7203 - accuracy: 0.7685 - val_loss: 0.4888 - val_accuracy: 0.8429\n",
      "Epoch 184/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7210 - accuracy: 0.7681 - val_loss: 0.4856 - val_accuracy: 0.8425\n",
      "Epoch 185/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7209 - accuracy: 0.7679 - val_loss: 0.4918 - val_accuracy: 0.8426\n",
      "Epoch 186/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7224 - accuracy: 0.7679 - val_loss: 0.4891 - val_accuracy: 0.8426\n",
      "Epoch 187/300\n",
      "621568/629699 [============================>.] - ETA: 0s - loss: 0.7218 - accuracy: 0.7675\n",
      "Epoch 00187: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7216 - accuracy: 0.7675 - val_loss: 0.4841 - val_accuracy: 0.8427\n",
      "Epoch 188/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7212 - accuracy: 0.7682 - val_loss: 0.4832 - val_accuracy: 0.8425\n",
      "Epoch 189/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7206 - accuracy: 0.7680 - val_loss: 0.4805 - val_accuracy: 0.8425\n",
      "Epoch 190/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7207 - accuracy: 0.7680 - val_loss: 0.4857 - val_accuracy: 0.8320\n",
      "Epoch 191/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7219 - accuracy: 0.7677 - val_loss: 0.5128 - val_accuracy: 0.8312\n",
      "Epoch 192/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7222 - accuracy: 0.7678 - val_loss: 0.4957 - val_accuracy: 0.8321\n",
      "Epoch 193/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7224 - accuracy: 0.7673 - val_loss: 0.5090 - val_accuracy: 0.8314\n",
      "Epoch 194/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7215 - accuracy: 0.7680 - val_loss: 0.5120 - val_accuracy: 0.8313\n",
      "Epoch 195/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7196 - accuracy: 0.7686 - val_loss: 0.4940 - val_accuracy: 0.8424\n",
      "Epoch 196/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7220 - accuracy: 0.7679 - val_loss: 0.4970 - val_accuracy: 0.8426\n",
      "Epoch 197/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "618496/629699 [============================>.] - ETA: 0s - loss: 0.7227 - accuracy: 0.7676\n",
      "Epoch 00197: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7228 - accuracy: 0.7676 - val_loss: 0.4883 - val_accuracy: 0.8425\n",
      "Epoch 198/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7208 - accuracy: 0.7679 - val_loss: 0.4994 - val_accuracy: 0.8424\n",
      "Epoch 199/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7213 - accuracy: 0.7678 - val_loss: 0.5081 - val_accuracy: 0.8315\n",
      "Epoch 200/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7225 - accuracy: 0.7678 - val_loss: 0.4859 - val_accuracy: 0.8431\n",
      "Epoch 201/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7219 - accuracy: 0.7680 - val_loss: 0.5039 - val_accuracy: 0.8312\n",
      "Epoch 202/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7210 - accuracy: 0.7678 - val_loss: 0.4833 - val_accuracy: 0.8432\n",
      "Epoch 203/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7207 - accuracy: 0.7679 - val_loss: 0.4977 - val_accuracy: 0.8424\n",
      "Epoch 204/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7203 - accuracy: 0.7681 - val_loss: 0.4839 - val_accuracy: 0.8426\n",
      "Epoch 205/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7230 - accuracy: 0.7674 - val_loss: 0.5067 - val_accuracy: 0.8313\n",
      "Epoch 206/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7221 - accuracy: 0.7677 - val_loss: 0.4820 - val_accuracy: 0.8426\n",
      "Epoch 207/300\n",
      "627200/629699 [============================>.] - ETA: 0s - loss: 0.7221 - accuracy: 0.7678\n",
      "Epoch 00207: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7221 - accuracy: 0.7678 - val_loss: 0.4843 - val_accuracy: 0.8428\n",
      "Epoch 208/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7211 - accuracy: 0.7676 - val_loss: 0.4778 - val_accuracy: 0.8432\n",
      "Epoch 209/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7217 - accuracy: 0.7678 - val_loss: 0.4819 - val_accuracy: 0.8426\n",
      "Epoch 210/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7217 - accuracy: 0.7681 - val_loss: 0.4873 - val_accuracy: 0.8425\n",
      "Epoch 211/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7203 - accuracy: 0.7684 - val_loss: 0.4863 - val_accuracy: 0.8426\n",
      "Epoch 212/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7221 - accuracy: 0.7678 - val_loss: 0.5022 - val_accuracy: 0.8428\n",
      "Epoch 213/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7206 - accuracy: 0.7679 - val_loss: 0.4984 - val_accuracy: 0.8425\n",
      "Epoch 214/300\n",
      "629699/629699 [==============================] - 19s 30us/sample - loss: 0.7203 - accuracy: 0.7682 - val_loss: 0.4963 - val_accuracy: 0.8425\n",
      "Epoch 215/300\n",
      "629699/629699 [==============================] - 18s 29us/sample - loss: 0.7200 - accuracy: 0.7686 - val_loss: 0.5048 - val_accuracy: 0.8320\n",
      "Epoch 216/300\n",
      "629699/629699 [==============================] - 18s 29us/sample - loss: 0.7205 - accuracy: 0.7680 - val_loss: 0.4900 - val_accuracy: 0.8425\n",
      "Epoch 217/300\n",
      "628224/629699 [============================>.] - ETA: 0s - loss: 0.7205 - accuracy: 0.7683\n",
      "Epoch 00217: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "629699/629699 [==============================] - 18s 29us/sample - loss: 0.7206 - accuracy: 0.7682 - val_loss: 0.4870 - val_accuracy: 0.8428\n",
      "Epoch 218/300\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.7211 - accuracy: 0.7681 - val_loss: 0.4879 - val_accuracy: 0.8313\n",
      "Epoch 219/300\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.7204 - accuracy: 0.7684 - val_loss: 0.5030 - val_accuracy: 0.8319\n",
      "Epoch 220/300\n",
      "629699/629699 [==============================] - 5s 8us/sample - loss: 0.7213 - accuracy: 0.7676 - val_loss: 0.5128 - val_accuracy: 0.8313\n",
      "Epoch 221/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7209 - accuracy: 0.7683 - val_loss: 0.4937 - val_accuracy: 0.8425\n",
      "Epoch 222/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7207 - accuracy: 0.7683 - val_loss: 0.4978 - val_accuracy: 0.8429\n",
      "Epoch 223/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7222 - accuracy: 0.7677 - val_loss: 0.4961 - val_accuracy: 0.8316\n",
      "Epoch 224/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7206 - accuracy: 0.7680 - val_loss: 0.4878 - val_accuracy: 0.8425\n",
      "Epoch 225/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7233 - accuracy: 0.7670 - val_loss: 0.5044 - val_accuracy: 0.8312\n",
      "Epoch 226/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7227 - accuracy: 0.7673 - val_loss: 0.4912 - val_accuracy: 0.8321\n",
      "Epoch 227/300\n",
      "626688/629699 [============================>.] - ETA: 0s - loss: 0.7216 - accuracy: 0.7678\n",
      "Epoch 00227: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7217 - accuracy: 0.7677 - val_loss: 0.4852 - val_accuracy: 0.8428\n",
      "Epoch 228/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7220 - accuracy: 0.7679 - val_loss: 0.4860 - val_accuracy: 0.8426\n",
      "Epoch 229/300\n",
      "629699/629699 [==============================] - 5s 8us/sample - loss: 0.7216 - accuracy: 0.7679 - val_loss: 0.5036 - val_accuracy: 0.8313\n",
      "Epoch 230/300\n",
      "629699/629699 [==============================] - 18s 29us/sample - loss: 0.7214 - accuracy: 0.7677 - val_loss: 0.4778 - val_accuracy: 0.8432\n",
      "Epoch 231/300\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.7211 - accuracy: 0.7679 - val_loss: 0.4870 - val_accuracy: 0.8427\n",
      "Epoch 232/300\n",
      "629699/629699 [==============================] - 19s 29us/sample - loss: 0.7215 - accuracy: 0.7677 - val_loss: 0.4899 - val_accuracy: 0.8319\n",
      "Epoch 233/300\n",
      "629699/629699 [==============================] - 18s 29us/sample - loss: 0.7211 - accuracy: 0.7678 - val_loss: 0.4848 - val_accuracy: 0.8425\n",
      "Epoch 234/300\n",
      "629699/629699 [==============================] - 18s 29us/sample - loss: 0.7196 - accuracy: 0.7685 - val_loss: 0.4870 - val_accuracy: 0.8426\n",
      "Epoch 235/300\n",
      "629699/629699 [==============================] - 15s 24us/sample - loss: 0.7204 - accuracy: 0.7683 - val_loss: 0.4926 - val_accuracy: 0.8432\n",
      "Epoch 236/300\n",
      "629699/629699 [==============================] - 4s 7us/sample - loss: 0.7220 - accuracy: 0.7677 - val_loss: 0.4870 - val_accuracy: 0.8418\n",
      "Epoch 237/300\n",
      "622592/629699 [============================>.] - ETA: 0s - loss: 0.7203 - accuracy: 0.7681\n",
      "Epoch 00237: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "629699/629699 [==============================] - 4s 7us/sample - loss: 0.7202 - accuracy: 0.7682 - val_loss: 0.4934 - val_accuracy: 0.8425\n",
      "Epoch 238/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7200 - accuracy: 0.7683 - val_loss: 0.4811 - val_accuracy: 0.8432\n",
      "Epoch 239/300\n",
      "629699/629699 [==============================] - 3s 6us/sample - loss: 0.7211 - accuracy: 0.7679 - val_loss: 0.4850 - val_accuracy: 0.8428\n",
      "Epoch 240/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7223 - accuracy: 0.7675 - val_loss: 0.4821 - val_accuracy: 0.8426\n",
      "Epoch 241/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7204 - accuracy: 0.7679 - val_loss: 0.4864 - val_accuracy: 0.8426\n",
      "Epoch 242/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7212 - accuracy: 0.7682 - val_loss: 0.4794 - val_accuracy: 0.8429\n",
      "Epoch 243/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7202 - accuracy: 0.7684 - val_loss: 0.5060 - val_accuracy: 0.8425\n",
      "Epoch 244/300\n",
      "629699/629699 [==============================] - 11s 18us/sample - loss: 0.7213 - accuracy: 0.7683 - val_loss: 0.4857 - val_accuracy: 0.8428\n",
      "Epoch 245/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629699/629699 [==============================] - 18s 29us/sample - loss: 0.7214 - accuracy: 0.7681 - val_loss: 0.4815 - val_accuracy: 0.8425\n",
      "Epoch 246/300\n",
      "629699/629699 [==============================] - 18s 29us/sample - loss: 0.7224 - accuracy: 0.7680 - val_loss: 0.5133 - val_accuracy: 0.8313\n",
      "Epoch 247/300\n",
      "627200/629699 [============================>.] - ETA: 0s - loss: 0.7214 - accuracy: 0.7681\n",
      "Epoch 00247: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.7214 - accuracy: 0.7681 - val_loss: 0.4860 - val_accuracy: 0.8429\n",
      "Epoch 248/300\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.7199 - accuracy: 0.7683 - val_loss: 0.5013 - val_accuracy: 0.8429\n",
      "Epoch 249/300\n",
      "629699/629699 [==============================] - 12s 19us/sample - loss: 0.7215 - accuracy: 0.7678 - val_loss: 0.4953 - val_accuracy: 0.8425\n",
      "Epoch 250/300\n",
      "629699/629699 [==============================] - 4s 7us/sample - loss: 0.7210 - accuracy: 0.7679 - val_loss: 0.4837 - val_accuracy: 0.8425\n",
      "Epoch 251/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7222 - accuracy: 0.7676 - val_loss: 0.4797 - val_accuracy: 0.8431\n",
      "Epoch 252/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7211 - accuracy: 0.7682 - val_loss: 0.4853 - val_accuracy: 0.8424\n",
      "Epoch 253/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7210 - accuracy: 0.7676 - val_loss: 0.4857 - val_accuracy: 0.8431\n",
      "Epoch 254/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7209 - accuracy: 0.7678 - val_loss: 0.5098 - val_accuracy: 0.8407\n",
      "Epoch 255/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7216 - accuracy: 0.7683 - val_loss: 0.4904 - val_accuracy: 0.8312\n",
      "Epoch 256/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7211 - accuracy: 0.7680 - val_loss: 0.4853 - val_accuracy: 0.8431\n",
      "Epoch 257/300\n",
      "625664/629699 [============================>.] - ETA: 0s - loss: 0.7209 - accuracy: 0.7682\n",
      "Epoch 00257: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "629699/629699 [==============================] - 5s 7us/sample - loss: 0.7209 - accuracy: 0.7682 - val_loss: 0.4850 - val_accuracy: 0.8425\n",
      "Epoch 258/300\n",
      "629699/629699 [==============================] - 15s 24us/sample - loss: 0.7220 - accuracy: 0.7674 - val_loss: 0.5043 - val_accuracy: 0.8425\n",
      "Epoch 259/300\n",
      "629699/629699 [==============================] - 19s 30us/sample - loss: 0.7220 - accuracy: 0.7677 - val_loss: 0.5059 - val_accuracy: 0.8313\n",
      "Epoch 260/300\n",
      "629699/629699 [==============================] - 19s 30us/sample - loss: 0.7215 - accuracy: 0.7681 - val_loss: 0.4843 - val_accuracy: 0.8426\n",
      "Epoch 261/300\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.7210 - accuracy: 0.7679 - val_loss: 0.4942 - val_accuracy: 0.8425\n",
      "Epoch 262/300\n",
      "629699/629699 [==============================] - 18s 29us/sample - loss: 0.7216 - accuracy: 0.7678 - val_loss: 0.4809 - val_accuracy: 0.8430\n",
      "Epoch 263/300\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.7217 - accuracy: 0.7678 - val_loss: 0.4861 - val_accuracy: 0.8428\n",
      "Epoch 264/300\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.7215 - accuracy: 0.7678 - val_loss: 0.4809 - val_accuracy: 0.8425\n",
      "Epoch 265/300\n",
      "629699/629699 [==============================] - 6s 9us/sample - loss: 0.7203 - accuracy: 0.7683 - val_loss: 0.4929 - val_accuracy: 0.8321\n",
      "Epoch 266/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7225 - accuracy: 0.7674 - val_loss: 0.4803 - val_accuracy: 0.8428\n",
      "Epoch 267/300\n",
      "621056/629699 [============================>.] - ETA: 0s - loss: 0.7225 - accuracy: 0.7674\n",
      "Epoch 00267: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7225 - accuracy: 0.7674 - val_loss: 0.4838 - val_accuracy: 0.8425\n",
      "Epoch 268/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7231 - accuracy: 0.7676 - val_loss: 0.4833 - val_accuracy: 0.8428\n",
      "Epoch 269/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7198 - accuracy: 0.7680 - val_loss: 0.4886 - val_accuracy: 0.8428\n",
      "Epoch 270/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7221 - accuracy: 0.7675 - val_loss: 0.4884 - val_accuracy: 0.8425\n",
      "Epoch 271/300\n",
      "629699/629699 [==============================] - 4s 7us/sample - loss: 0.7213 - accuracy: 0.7681 - val_loss: 0.4906 - val_accuracy: 0.8426\n",
      "Epoch 272/300\n",
      "629699/629699 [==============================] - 4s 7us/sample - loss: 0.7211 - accuracy: 0.7677 - val_loss: 0.4820 - val_accuracy: 0.8425\n",
      "Epoch 273/300\n",
      "629699/629699 [==============================] - 11s 17us/sample - loss: 0.7201 - accuracy: 0.7681 - val_loss: 0.4875 - val_accuracy: 0.8425\n",
      "Epoch 274/300\n",
      "629699/629699 [==============================] - 18s 29us/sample - loss: 0.7213 - accuracy: 0.7677 - val_loss: 0.5080 - val_accuracy: 0.8313\n",
      "Epoch 275/300\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.7194 - accuracy: 0.7683 - val_loss: 0.4827 - val_accuracy: 0.8426\n",
      "Epoch 276/300\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.7209 - accuracy: 0.7679 - val_loss: 0.4900 - val_accuracy: 0.8425\n",
      "Epoch 277/300\n",
      "628736/629699 [============================>.] - ETA: 0s - loss: 0.7205 - accuracy: 0.7679\n",
      "Epoch 00277: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.7205 - accuracy: 0.7679 - val_loss: 0.5125 - val_accuracy: 0.8313\n",
      "Epoch 278/300\n",
      "629699/629699 [==============================] - 11s 17us/sample - loss: 0.7231 - accuracy: 0.7671 - val_loss: 0.4967 - val_accuracy: 0.8317\n",
      "Epoch 279/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7205 - accuracy: 0.7682 - val_loss: 0.4869 - val_accuracy: 0.8430\n",
      "Epoch 280/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7217 - accuracy: 0.7678 - val_loss: 0.4989 - val_accuracy: 0.8313\n",
      "Epoch 281/300\n",
      "629699/629699 [==============================] - 3s 6us/sample - loss: 0.7212 - accuracy: 0.7680 - val_loss: 0.4941 - val_accuracy: 0.8424\n",
      "Epoch 282/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7217 - accuracy: 0.7680 - val_loss: 0.4935 - val_accuracy: 0.8314\n",
      "Epoch 283/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7198 - accuracy: 0.7688 - val_loss: 0.5046 - val_accuracy: 0.8431\n",
      "Epoch 284/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7211 - accuracy: 0.7679 - val_loss: 0.4921 - val_accuracy: 0.8318\n",
      "Epoch 285/300\n",
      "629699/629699 [==============================] - 3s 5us/sample - loss: 0.7207 - accuracy: 0.7681 - val_loss: 0.4813 - val_accuracy: 0.8427\n",
      "Epoch 286/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7213 - accuracy: 0.7680 - val_loss: 0.4997 - val_accuracy: 0.8314\n",
      "Epoch 287/300\n",
      "627712/629699 [============================>.] - ETA: 0s - loss: 0.7198 - accuracy: 0.7682\n",
      "Epoch 00287: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "629699/629699 [==============================] - 4s 7us/sample - loss: 0.7198 - accuracy: 0.7682 - val_loss: 0.4833 - val_accuracy: 0.8428\n",
      "Epoch 288/300\n",
      "629699/629699 [==============================] - 14s 22us/sample - loss: 0.7211 - accuracy: 0.7678 - val_loss: 0.4805 - val_accuracy: 0.8425\n",
      "Epoch 289/300\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.7208 - accuracy: 0.7681 - val_loss: 0.4980 - val_accuracy: 0.8424\n",
      "Epoch 290/300\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.7202 - accuracy: 0.7679 - val_loss: 0.4858 - val_accuracy: 0.8425\n",
      "Epoch 291/300\n",
      "629699/629699 [==============================] - 18s 29us/sample - loss: 0.7226 - accuracy: 0.7673 - val_loss: 0.4961 - val_accuracy: 0.8425\n",
      "Epoch 292/300\n",
      "629699/629699 [==============================] - 18s 28us/sample - loss: 0.7209 - accuracy: 0.7681 - val_loss: 0.5083 - val_accuracy: 0.8425\n",
      "Epoch 293/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629699/629699 [==============================] - 10s 15us/sample - loss: 0.7204 - accuracy: 0.7684 - val_loss: 0.4940 - val_accuracy: 0.8321\n",
      "Epoch 294/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7220 - accuracy: 0.7676 - val_loss: 0.4851 - val_accuracy: 0.8425\n",
      "Epoch 295/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7220 - accuracy: 0.7679 - val_loss: 0.4813 - val_accuracy: 0.8425\n",
      "Epoch 296/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7208 - accuracy: 0.7684 - val_loss: 0.4894 - val_accuracy: 0.8425\n",
      "Epoch 297/300\n",
      "619008/629699 [============================>.] - ETA: 0s - loss: 0.7197 - accuracy: 0.7684\n",
      "Epoch 00297: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "629699/629699 [==============================] - 3s 6us/sample - loss: 0.7196 - accuracy: 0.7685 - val_loss: 0.4928 - val_accuracy: 0.8425\n",
      "Epoch 298/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7216 - accuracy: 0.7682 - val_loss: 0.4916 - val_accuracy: 0.8425\n",
      "Epoch 299/300\n",
      "629699/629699 [==============================] - 4s 6us/sample - loss: 0.7219 - accuracy: 0.7678 - val_loss: 0.4798 - val_accuracy: 0.8428\n",
      "Epoch 300/300\n",
      "629699/629699 [==============================] - 3s 6us/sample - loss: 0.7217 - accuracy: 0.7679 - val_loss: 0.4993 - val_accuracy: 0.8315\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "monitor = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\",factor=0.5,mode=\"min\",patience=10,verbose=1,min_lr=1e-7)\n",
    "checkpoint = ModelCheckpoint('DNN_Best-Model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=300, batch_size=512, callbacks=[monitor, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125248/279867 [============>.................] - ETA: 18s - loss: 0.4716 - accuracy: 0.8444"
     ]
    }
   ],
   "source": [
    "# Load the best saved model\n",
    "best_model = load_model('DNN_Best-Model.h5')\n",
    "\n",
    "# Evaluate the best saved model\n",
    "score = best_model.evaluate(x_val, y_val)\n",
    "print('')\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 16))\n",
    "\n",
    "ax.plot(history.history['loss'], label='train')\n",
    "ax.plot(history.history['val_loss'], label='test')\n",
    "ax.set_title('Model Loss')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ihn2qm186fxn"
   },
   "source": [
    "-------------------------------------\n",
    "\n",
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5hVv8L2EN_XN",
    "outputId": "6d8a189e-7051-42a6-dd0d-b626eb55d212"
   },
   "outputs": [],
   "source": [
    "pred = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Random state definition\n",
    "random_state=42\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Metrics for the classification\n",
    "def compute_metrics(pred, y_val):\n",
    "    predict_classes = np.argmax(pred, axis = 1)\n",
    "    expected_classes = np.argmax(y_val, axis = 1)\n",
    "    \n",
    "    correct = metrics.accuracy_score(expected_classes, predict_classes)\n",
    "    print(f\"Accuracy: {correct}\")\n",
    "    \n",
    "    recall = metrics.recall_score(expected_classes, predict_classes, average = 'weighted')    \n",
    "    print(f\"Recall: {recall}\")\n",
    "       \n",
    "    precision = metrics.precision_score(expected_classes, predict_classes, average = 'weighted')\n",
    "    print(f\"Precision: {precision}\")\n",
    "    \n",
    "    f1score = metrics.f1_score(expected_classes, predict_classes, average = 'weighted')\n",
    "    print(f\"F1-Score: {f1score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Trees Classifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extra_tree_classifier(x_train, y_train, x_columns):\n",
    "    # Usage of ExtraTreesClassifier for feature selection\n",
    "    extra_tree_forest = ExtraTreesClassifier(n_estimators=5, criterion='entropy', max_features=2, random_state=random_state)\n",
    "    extra_tree_forest.fit(x_train, y_train)\n",
    "    feature_importances = extra_tree_forest.feature_importances_\n",
    "    feature_importance_normalized = np.std([tree.feature_importances_ for tree in  extra_tree_forest.estimators_], axis = 0)\n",
    "\n",
    "    # Plot the feature importances\n",
    "    plt.rcParams[\"figure.figsize\"] = (70, 40)\n",
    "    plt.bar(x_columns, feature_importance_normalized, align='center')\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Feature Importance')\n",
    "    plt.title('Comparison of different feature importances in the current dataset')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjwPGzH7cXls"
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ATTACKS = [\n",
    "    'Arp Spoofing', 'BotNet DDOS', \n",
    "    'HTTP Flood', 'ICMP Flood', \n",
    "    'MQTT Flood', 'Normal', \n",
    "    'Port Scanning', 'TCP Flood', \n",
    "    'UDP Flood'\n",
    "]\n",
    "    \n",
    "def conf_matrix(y_test, prediction, array_dimension = 2):\n",
    "    if (array_dimension == 2):\n",
    "        predict_classes = np.argmax(prediction, axis = 1)\n",
    "        expected_classes = np.argmax(y_test, axis = 1)\n",
    "    elif (array_dimension == 1):\n",
    "        predict_classes = prediction\n",
    "        expected_classes = y_test\n",
    "    \n",
    "    cm = confusion_matrix(expected_classes, predict_classes)\n",
    "    cmd = ConfusionMatrixDisplay(cm, display_labels=ATTACKS)\n",
    "\n",
    "    # Plot size\n",
    "    fig, ax = plt.subplots(figsize=(11,11))\n",
    "    \n",
    "    cmd.plot(ax=ax, cmap=plt.cm.Blues, colorbar=False)\n",
    "    \n",
    "    # Add axis labels and rotate them\n",
    "    ax.set_xlabel('Predicted labels', rotation=0, labelpad=20, fontsize=11)\n",
    "    ax.set_ylabel('True labels', rotation=90, labelpad=20, fontsize=11)\n",
    "\n",
    "    ax.set_xticks([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "    ax.set_yticks([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "    ax.set_xticklabels(ATTACKS)\n",
    "    ax.set_yticklabels(ATTACKS)\n",
    "        \n",
    "    ax.tick_params(axis='x', pad=35)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha='center', va='center')\n",
    "    ax.tick_params(axis='y', pad=35)\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=40, ha='center', va='center')\n",
    "    \n",
    "    # Adjust colorbar size\n",
    "    cax = plt.gcf().axes[-1]\n",
    "    cax.tick_params(labelsize=11)  # Adjust the font size of colorbar labels\n",
    "    cax = fig.add_axes([ax.get_position().x1+0.03,ax.get_position().y0,0.02,ax.get_position().height])\n",
    "    plt.colorbar(cmd.im_, cax=cax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7opqgASr6fxn"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None, normalize=False):\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshowac(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=90)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'[METRICS] Decision Tree Classifier')\n",
    "compute_metrics(y_val, pred)\n",
    "\n",
    "print(f'\\n[CMATRIX] Decision Tree Confusion Matrix')\n",
    "conf_matrix(y_val, pred, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
